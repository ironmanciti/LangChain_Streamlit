{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TypDOiTPbjE"
   },
   "source": [
    "# LangChain ê¸°ëŠ¥ ì†Œê°œ\n",
    "\n",
    "- 1. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ê³¼ ë¡œë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ì²´ì¸ êµ¬ì„±  \n",
    "- 2. Runnableê³¼ LangChain í‘œí˜„ ì–¸ì–´  \n",
    "- 3. RAG ì²´ì¸ êµ¬ì¶•  \n",
    "- 4. ì²´ì¸ì´ ë‹¬ë¦° ë„êµ¬ ì‚¬ìš©  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOUiSLMxPzZY"
   },
   "source": [
    "## LangChain ê°œìš”\n",
    "\n",
    "LangChainìœ¼ë¡œ ê°œë°œëœ ì• í”Œë¦¬ì¼€ì´ì…˜ì€ ì¼ë°˜ì ìœ¼ë¡œ ì„¸ ê°€ì§€ ë²”ì£¼ë¡œ ë‚˜ë‰©ë‹ˆë‹¤.\n",
    "\n",
    "- ì±—ë´‡ : LLMì„ ì´ìš©í•˜ì—¬ ë³´ë‹¤ ì§€ëŠ¥ì ì¸ ë§ˆì¼€íŒ…, ê³ ê° ì§€ì›, êµìœ¡ ìƒí˜¸ì‘ìš© êµ¬í˜„.\n",
    "- ê²€ìƒ‰ ì¦ê°• ìƒì„±(RAG) Q&A : ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ìš”ì•½, ë°ì´í„° ë¶„ì„, ì™¸ë¶€ ì†ŒìŠ¤ë¥¼ ì°¸ì¡°í•˜ì—¬ ì½”ë“œ ìƒì„±ì— ì‚¬ìš©.\n",
    "- ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì€ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì„¤ì •ê³¼ ì¸ê°„ ìƒí˜¸ì‘ìš©ì„ í¬í•¨í•˜ë©° ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ ìœ„í•´ LangGraphë¥¼ í™œìš©. ê³µê¸‰ë§ ê´€ë¦¬ ë° ìš´ì˜ ìµœì í™”ì™€ ê°™ì€ ë¶„ì•¼ì— ì ìš© ê°€ëŠ¥.\n",
    "\n",
    "LangChainì€ ìì—°ì–´ë¥¼ ì‹¤í–‰ ê°€ëŠ¥í•œ í”„ë¡œê·¸ë¨ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” íŒŒì´í”„ë¼ì¸ì„ ìƒì„±í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì²´ì¸ì„ í™œìš©í•¨ìœ¼ë¡œì¨ ì‚¬ìš©ìëŠ” ìì—°ì–´ë¥¼ ì…ë ¥í•˜ê³  ë³´ê³ ì„œ, ë¶„ì„ ë˜ëŠ” ì»´í“¨í„° í”„ë¡œê·¸ë¨ê³¼ ê°™ì€ ì¶œë ¥ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FXRwb1YcsW1j",
    "outputId": "7e5cff59-b6f5-446d-e3cc-93891014f5c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env íŒŒì¼ì—ì„œ API í‚¤ë¥¼ ë¡œë“œ\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì‚¬ìš© ê°€ëŠ¥í•œ LLM ëª©ë¡ ì¡°íšŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gpt-4.5-preview', 'gpt-4.5-preview-2025-02-27', 'gpt-4o-mini-audio-preview-2024-12-17', 'dall-e-3', 'dall-e-2', 'gpt-4o-audio-preview-2024-10-01', 'gpt-4o-audio-preview', 'gpt-4o-mini-realtime-preview-2024-12-17', 'gpt-4o-mini-realtime-preview', 'o1-mini-2024-09-12', 'o1-mini', 'omni-moderation-latest', 'gpt-4o-mini-audio-preview', 'omni-moderation-2024-09-26', 'whisper-1', 'gpt-4o-realtime-preview-2024-10-01', 'babbage-002', 'gpt-4-turbo-preview', 'chatgpt-4o-latest', 'tts-1-hd-1106', 'text-embedding-3-large', 'gpt-4-0125-preview', 'gpt-4o-audio-preview-2024-12-17', 'gpt-4', 'gpt-4o-2024-05-13', 'tts-1-hd', 'o1-preview', 'o1-preview-2024-09-12', 'gpt-4o-2024-11-20', 'gpt-3.5-turbo-instruct-0914', 'gpt-4o-mini-2024-07-18', 'gpt-4o-mini', 'tts-1', 'tts-1-1106', 'davinci-002', 'gpt-3.5-turbo-1106', 'gpt-4-turbo', 'gpt-3.5-turbo-instruct', 'o1', 'gpt-4o-2024-08-06', 'gpt-3.5-turbo-0125', 'gpt-4o-realtime-preview-2024-12-17', 'gpt-3.5-turbo', 'gpt-4-turbo-2024-04-09', 'gpt-4o-realtime-preview', 'gpt-3.5-turbo-16k', 'gpt-4o', 'text-embedding-3-small', 'gpt-4-1106-preview', 'text-embedding-ada-002', 'o3-mini-2025-01-31', 'gpt-4-0613', 'o3-mini', 'o1-2024-12-17']\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "models = openai.models.list()\n",
    "\n",
    "# ê° ëª¨ë¸ì˜ ID ì¶œë ¥\n",
    "print([model.id for model in models])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEi1gjmpW8ab"
   },
   "source": [
    "## LLM ì—°ê²°\n",
    "\n",
    "OpenAI ë° Anthropic APIì— ì—°ê²°í•©ë‹ˆë‹¤. ì›í•˜ëŠ” ëª¨ë¸ì„ ì§€ì •í•˜ì—¬ ChatOpenAI ë° ChatAnthropic í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“­ë‹ˆë‹¤. llm_claude3 ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°„ë‹¨í•œ ì¿¼ë¦¬ë¥¼ í˜¸ì¶œí•˜ì—¬ ì„¤ì •ì„ í™•ì¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jSY2uUq8ssCp"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# from langchain_anthropic import ChatAnthropic\n",
    "# llm = ChatAnthropic(model=\"claude-3-5-haiku-20241022\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "id": "NFWMvMRitGX-",
    "outputId": "9eff0799-8467-4610-ee63-d3b945657364"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChainì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ê°„ì†Œí™”í•˜ëŠ” ì˜¤í”ˆ ì†ŒìŠ¤ Python í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ì£¼ìš” íŠ¹ì§•ê³¼ ê¸°ëŠ¥ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "\n",
      "1. ì£¼ìš” ëª©ì \n",
      "- AI ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ ì´‰ì§„\n",
      "- LLM ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ êµ¬ì¶• ë³µì¡ì„± ê°ì†Œ\n",
      "- ë‹¤ì–‘í•œ AI ì‘ì—… ì§€ì›\n",
      "\n",
      "2. í•µì‹¬ êµ¬ì„± ìš”ì†Œ\n",
      "- Prompt ê´€ë¦¬\n",
      "- ë©”ëª¨ë¦¬ ê¸°ëŠ¥\n",
      "- ë°ì´í„° ìƒ‰ì¸\n",
      "- ì²´ì¸(Chain) ìƒì„±\n",
      "- ì—ì´ì „íŠ¸ ê°œë°œ\n",
      "- ì™¸ë¶€ ë„êµ¬ í†µí•©\n",
      "\n",
      "3. ì£¼ìš” ê¸°ëŠ¥\n",
      "- ë¬¸ì„œ ë¡œë”© ë° ì²˜ë¦¬\n",
      "- í…ìŠ¤íŠ¸ ë¶„í• \n",
      "- ë²¡í„° ì €ì¥ì†Œ ì—°ê²°\n",
      "- API í†µí•©\n",
      "- ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„\n",
      "\n",
      "4. ì§€ì›í•˜ëŠ” ì£¼ìš” ëª¨ë¸\n",
      "- OpenAI GPT\n",
      "- Hugging Face ëª¨ë¸\n",
      "- Google Vertex AI\n",
      "- Anthropic Claude\n",
      "\n",
      "5. í™œìš© ì‚¬ë¡€\n",
      "- ì±—ë´‡\n",
      "- ë¬¸ì„œ ë¶„ì„\n",
      "- ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ\n",
      "- ë°ì´í„° ìš”ì•½\n",
      "- ì½”ë“œ ìƒì„±\n",
      "\n",
      "LangChainì€ AI ê°œë°œìë“¤ì—ê²Œ ê°•ë ¥í•˜ê³  ìœ ì—°í•œ ê°œë°œ ë„êµ¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"í•œêµ­ì–´ë¡œ LangChain ì— ëŒ€í•´ì„œ ì„¤ëª…í•´ì¤˜\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Messages\n",
    "- AIMessage: AI(ì¸ê³µì§€ëŠ¥) ëª¨ë¸ì´ ìƒì„±í•œ ë©”ì‹œì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.  \n",
    "ì˜ˆë¥¼ ë“¤ì–´, AI ëª¨ë¸ì´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì‘ë‹µì„ ìƒì„±í•  ë•Œ ì´ í´ë˜ìŠ¤ê°€ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
    "\n",
    "- HumanMessage: ì‚¬ëŒ(ì‚¬ìš©ì)ì´ ìƒì„±í•œ ë©”ì‹œì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.  \n",
    "ì‚¬ìš©ìê°€ ì…ë ¥í•œ í…ìŠ¤íŠ¸ë‚˜ ì§ˆë¬¸ ë“±ì´ ì—¬ê¸°ì— í•´ë‹¹ë©ë‹ˆë‹¤.\n",
    "\n",
    "- StemMessage: AI í–‰ë™ì„ í”„ë¼ì´ë°í•˜ê¸° ìœ„í•œ ë©”ì‹œì§€.\n",
    "ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì¼ë ¨ì˜ ì…ë ¥ ë©”ì‹œì§€ ì¤‘ ì²« ë²ˆì§¸ë¡œ ì „ë‹¬ë©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ëŒ€í™”ì˜ íë¦„ì„ ì œì–´í•˜ê±°ë‚˜ íŠ¹ì • ì§€ì¹¨ì„ ì œê³µí•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "ë§ˆì¹˜ ë‹¤ì„¯ì‚´ ë¨¹ì€ ì–´ë¦°ì•„ì´ì—ê²Œ ì„¤ëª…í•˜ë“¯ì´ í•œêµ­ì–´ë¡œ ì‰½ê²Œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "LangChainì´ ë¬´ì—‡ì¸ê°€ìš”?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gYhhibXZSGTF",
    "outputId": "3d276aa8-39a2-4b85-b9aa-5bb43729783b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ì–´ë¦° ì•„ì´ì—ê²Œ ì„¤ëª…í•˜ë“¯ì´ LangChainì„ ì•Œë ¤ë“œë¦´ê²Œìš”! ğŸ¤–\\n\\nLangChainì€ ë§ˆì¹˜ ë ˆê³  ë¸”ë¡ ê°™ì€ ë©‹ì§„ í”„ë¡œê·¸ë¨ì´ì—ìš”. ì´ íŠ¹ë³„í•œ ë ˆê³  ë¸”ë¡ë“¤ì€ ì¸ê³µì§€ëŠ¥(AI)ê³¼ ëŒ€í™”í•˜ê³  ë©‹ì§„ ì¼ë“¤ì„ í•  ìˆ˜ ìˆê²Œ ë„ì™€ì¤ë‹ˆë‹¤.\\n\\nì˜ˆë¥¼ ë“¤ì–´:\\nâ€¢ ì±—GPT ê°™ì€ AIë¥¼ ë” ë˜‘ë˜‘í•˜ê²Œ ë§Œë“¤ì–´ìš”\\nâ€¢ ë³µì¡í•œ ì§ˆë¬¸ì— ë‹µí•  ìˆ˜ ìˆê²Œ í•´ì¤˜ìš”\\nâ€¢ ì—¬ëŸ¬ AI ë„êµ¬ë“¤ì„ ì‰½ê²Œ ì—°ê²°í•  ìˆ˜ ìˆì–´ìš”\\n\\në§ˆì¹˜ ë ˆê³ ë¡œ ì›í•˜ëŠ” ëª¨ì–‘ì„ ë§Œë“¤ ë“¯ì´, LangChainìœ¼ë¡œ ê°œë°œìë“¤ì€ AIë¡œ ì›í•˜ëŠ” ê¸°ëŠ¥ì„ ë§Œë“¤ ìˆ˜ ìˆì–´ìš”. \\n\\nê°„ë‹¨í•˜ê²Œ ë§í•´, LangChainì€ AIì™€ ë†€ ìˆ˜ ìˆëŠ” ë©‹ì§„ ë†€ì´ë„êµ¬ë¼ê³  ìƒê°í•˜ë©´ ë¼ìš”! ğŸŒŸ', additional_kwargs={}, response_metadata={'id': 'msg_01LDbx9Sx2w3suk77GBcsdHe', 'model': 'claude-3-5-haiku-20241022', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 66, 'output_tokens': 312}}, id='run-178266a3-6ba1-4525-9c69-855cd68eee1d-0', usage_metadata={'input_tokens': 66, 'output_tokens': 312, 'total_tokens': 378, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# systemê³¼ human/user ë©”ì‹œì§€ë¥¼ ì´ìš©í•œ ê¸°ë³¸ ìš”ì²­\n",
    "\n",
    "message = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=user_prompt),\n",
    "]\n",
    "\n",
    "response = llm.invoke(message)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "AK7Xe8wnSQGa",
    "outputId": "c314481f-1ecb-4337-bf3c-08c274701b7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì–´ë¦° ì•„ì´ì—ê²Œ ì„¤ëª…í•˜ë“¯ì´ LangChainì„ ì•Œë ¤ë“œë¦´ê²Œìš”! ğŸ¤–\n",
      "\n",
      "LangChainì€ ë§ˆì¹˜ ë ˆê³  ë¸”ë¡ ê°™ì€ ë©‹ì§„ í”„ë¡œê·¸ë¨ì´ì—ìš”. ì´ íŠ¹ë³„í•œ ë ˆê³  ë¸”ë¡ë“¤ì€ ì¸ê³µì§€ëŠ¥(AI)ê³¼ ëŒ€í™”í•˜ê³  ë©‹ì§„ ì¼ë“¤ì„ í•  ìˆ˜ ìˆê²Œ ë„ì™€ì¤ë‹ˆë‹¤.\n",
      "\n",
      "ì˜ˆë¥¼ ë“¤ì–´:\n",
      "â€¢ ì±—GPT ê°™ì€ AIë¥¼ ë” ë˜‘ë˜‘í•˜ê²Œ ë§Œë“¤ì–´ìš”\n",
      "â€¢ ë³µì¡í•œ ì§ˆë¬¸ì— ë‹µí•  ìˆ˜ ìˆê²Œ í•´ì¤˜ìš”\n",
      "â€¢ ì—¬ëŸ¬ AI ë„êµ¬ë“¤ì„ ì‰½ê²Œ ì—°ê²°í•  ìˆ˜ ìˆì–´ìš”\n",
      "\n",
      "ë§ˆì¹˜ ë ˆê³ ë¡œ ì›í•˜ëŠ” ëª¨ì–‘ì„ ë§Œë“¤ ë“¯ì´, LangChainìœ¼ë¡œ ê°œë°œìë“¤ì€ AIë¡œ ì›í•˜ëŠ” ê¸°ëŠ¥ì„ ë§Œë“¤ ìˆ˜ ìˆì–´ìš”. \n",
      "\n",
      "ê°„ë‹¨í•˜ê²Œ ë§í•´, LangChainì€ AIì™€ ë†€ ìˆ˜ ìˆëŠ” ë©‹ì§„ ë†€ì´ë„êµ¬ë¼ê³  ìƒê°í•˜ë©´ ë¼ìš”! ğŸŒŸ\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ìˆ˜í•™ ì„ ìƒë‹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81ì„ 9ë¡œ ë‚˜ëˆ„ë©´ 9ì…ë‹ˆë‹¤.\n",
      "\n",
      "ê³„ì‚° ê³¼ì •:\n",
      "81 Ã· 9 = 9\n",
      "\n",
      "í™•ì¸ ë°©ë²•:\n",
      "9 Ã— 9 = 81 ì´ë¯€ë¡œ ì •ë‹µì€ 9ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"ë‹¹ì‹ ì€  ìˆ˜í•™ ë¬¸ì œë¥¼ ì˜ í’€ì–´ì£¼ëŠ” ë„ì›€ë˜ëŠ” assistant ì…ë‹ˆë‹¤.\"),\n",
    "    HumanMessage(content=\"81ì„ 9ë¡œ ë‚˜ëˆ„ë©´ ëª‡ì¸ê°€ìš”?\")\n",
    "]\n",
    "\n",
    "result = llm.invoke(messages)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 ê³±í•˜ê¸° 5ëŠ” 50ì…ë‹ˆë‹¤.\n",
      "\n",
      "ê³„ì‚° ê³¼ì •:\n",
      "10 Ã— 5 = 50\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"ë‹¹ì‹ ì€  ìˆ˜í•™ ë¬¸ì œë¥¼ ì˜ í’€ì–´ì£¼ëŠ” ë„ì›€ë˜ëŠ” assistant ì…ë‹ˆë‹¤.\"),\n",
    "    HumanMessage(content=\"81ì„ 9ë¡œ ë‚˜ëˆ„ë©´ ëª‡ì¸ê°€ìš”?\"),\n",
    "    AIMessage(content=\"81ì„ 9ë¡œ ë‚˜ëˆ„ë©´ 9ì…ë‹ˆë‹¤.\"),\n",
    "    HumanMessage(content=\"10 ê³±í•˜ê¸° 5ëŠ” ì–¼ë§ˆì¸ê°€ìš”?\")\n",
    "]\n",
    "\n",
    "result = llm.invoke(messages)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fi6_NvW3MaE"
   },
   "source": [
    "##  1. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ê³¼ ë¡œë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ì²´ì¸ êµ¬ì„±  \n",
    "\n",
    "- LLMì—ì„œì˜ Chain ì€ Data Processing ì—ì„œì˜ Pipeline ê³¼ ìœ ì‚¬í•œ ê°œë…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G447ezl5X5ye"
   },
   "source": [
    "### ì²´ì¸ì˜ êµ¬ì„± ìš”ì†Œ - Runnables\n",
    "- í”„ë¡¬í”„íŠ¸ : LLMì˜ ì‘ë‹µì„ ì•ˆë‚´í•˜ëŠ” í…œí”Œë¦¿  \n",
    "- LLM ë˜ëŠ” ì±„íŒ… ëª¨ë¸ : í”„ë¡¬í”„íŠ¸ì— ë”°ë¼ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì—”ì§„  \n",
    "- ì¶œë ¥ íŒŒì„œ : LLMì˜ ì¶œë ¥ì„ íŒŒì‹±í•˜ëŠ” ë„êµ¬  \n",
    "- ë„êµ¬ : LLMì´ APIì—ì„œ ì¶”ê°€ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê±°ë‚˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ LLMì„ ì—ì´ì „íŠ¸ë¡œ ì „í™˜í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” í™•ì¥ ê¸°ëŠ¥  \n",
    "- ì¼ë°˜ í•¨ìˆ˜ : ì„œë¡œ ì—°ê²°ë  ìˆ˜ ìˆëŠ” ì¶”ê°€ì ì¸ ì¼ë°˜ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Yt1j9kXR1h53"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='\\n    ë‹¹ì‹ ì€ AI ì£¼ì œë¥¼ ì„¤ëª…í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¤ìŒ ì…ë ¥ì´ ì£¼ì–´ì§€ë©´:\\n    {topic}\\n    ì£¼ì–´ì§„ ì£¼ì œì— ëŒ€í•œ ì„¤ëª…ì„ ì œê³µí•˜ì„¸ìš”.\\n')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# simple prompt template ìƒì„±\n",
    "# {topic} ë³€ìˆ˜ì—ì„œ ì‚¬ìš©ìì˜ query ê°€ ëŒ€ì²´ëœë‹¤.\n",
    "prompt_template = \"\"\"\n",
    "    ë‹¹ì‹ ì€ AI ì£¼ì œë¥¼ ì„¤ëª…í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¤ìŒ ì…ë ¥ì´ ì£¼ì–´ì§€ë©´:\n",
    "    {topic}\n",
    "    ì£¼ì–´ì§„ ì£¼ì œì— ëŒ€í•œ ì„¤ëª…ì„ ì œê³µí•˜ì„¸ìš”.\n",
    "\"\"\"\n",
    "\n",
    "# prompt template ì„ ì´ìš©í•˜ì—¬ prompt ìƒì„± - old style\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "# prompt template ì„ ì´ìš©í•˜ì—¬ prompt ìƒì„± - new style\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "id": "0vikrfE53yDk",
    "outputId": "adf40012-9025-4a1a-a591-da9fd5b4883c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChainì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ ì‚¬ìš©í•˜ì—¬ ë” ë³µì¡í•˜ê³  ìœ ìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•˜ê¸° ìœ„í•œ ì˜¤í”ˆ ì†ŒìŠ¤ Python í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ì£¼ìš” íŠ¹ì§•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "\n",
      "1. ì£¼ìš” ê¸°ëŠ¥\n",
      "- LLMê³¼ ìƒí˜¸ì‘ìš©ì„ ì‰½ê²Œ ë§Œë“œëŠ” ë„êµ¬ ì œê³µ\n",
      "- ë‹¤ì–‘í•œ AI ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ ì§€ì›\n",
      "- í”„ë¡¬í”„íŠ¸ ê´€ë¦¬, ë©”ëª¨ë¦¬, ì²´ì¸ ë“±ì˜ ê¸°ëŠ¥ í¬í•¨\n",
      "\n",
      "2. ì£¼ìš” ì‚¬ìš© ì‚¬ë¡€\n",
      "- ì±—ë´‡ ê°œë°œ\n",
      "- ë¬¸ì„œ ë¶„ì„\n",
      "- ë°ì´í„° ìš”ì•½\n",
      "- ì§ˆë¬¸-ë‹µë³€ ì‹œìŠ¤í…œ\n",
      "- AI ì—ì´ì „íŠ¸ ìƒì„±\n",
      "\n",
      "3. ì£¼ìš” ì¥ì \n",
      "- ìœ ì—°ì„±\n",
      "- ì‰¬ìš´ í†µí•©\n",
      "- ë‹¤ì–‘í•œ AI ëª¨ë¸ ì§€ì›\n",
      "- ë³µì¡í•œ AI ì›Œí¬í”Œë¡œìš° êµ¬í˜„ ìš©ì´\n",
      "\n",
      "ê°œë°œìë“¤ì´ AI ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë” íš¨ìœ¨ì ìœ¼ë¡œ êµ¬ì¶•í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ê°•ë ¥í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# pipe operator \"|\"\" ë¥¼ ì´ìš©í•˜ì—¬ í•˜ë‚˜ ì´ìƒì˜ chain ì„ ê²°í•©\n",
    "chain = prompt | llm\n",
    "\n",
    "print(chain.invoke({\"topic\": \"LangChainì´ ë­ì˜ˆìš”?\"}).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJzXNZCnZRCr"
   },
   "source": [
    "### document loader ë¥¼ ì´ìš©í•˜ì—¬ script ìš”ì•½\n",
    "ë” ê³ ê¸‰ ì²´ì¸ì„ ë§Œë“¤ê¸° ìœ„í•´ LangChainì„ ì‚¬ìš©í•˜ì—¬ YouTube ë¹„ë””ì˜¤ë¥¼ í•„ì‚¬í•©ë‹ˆë‹¤. ë¨¼ì € YouTube Transcript APIë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ LangChainì˜ ì»¤ë®¤ë‹ˆí‹° ë¬¸ì„œ ë¡œë”ì—ì„œ YouTube Loaderë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. ë¡œë”ì— ë¹„ë””ì˜¤ URLì„ ì „ë‹¬í•˜ë©´ í•„ì‚¬ë³¸ì„ ì¶”ì¶œí•  ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ë¬¸ì„œ ëª©ë¡ìœ¼ë¡œ ë°˜í™˜ë©ë‹ˆë‹¤. ì›ì‹œ í•„ì‚¬ë³¸ì„ ì–»ìœ¼ë ¤ë©´ ì´ëŸ¬í•œ ë¬¸ì„œì—ì„œ í˜ì´ì§€ ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•˜ê±°ë‚˜ ë¬¸ì„œë¥¼ ì²´ì¸ì— ì§ì ‘ ì „ë‹¬í•˜ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "L1HMGE6k4mQp"
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade --quiet youtube-transcript-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "3r04ZHou46-F"
   },
   "outputs": [],
   "source": [
    "# LangChain ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ì œê³µí•˜ëŠ” Youtube Loaderë¥¼ ì„í¬íŠ¸\n",
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "# YoutubeLoaderë¥¼ ì‚¬ìš©í•˜ì—¬ ìœ íŠœë¸Œ URLì—ì„œ ë¡œë”ë¥¼ ìƒì„±\n",
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=AOEGOhkGtjI\", add_video_info=False\n",
    ")\n",
    "\n",
    "# ë¹„ë””ì˜¤ì˜ ìë§‰ì„ ë¬¸ì„œë¡œ ë¡œë“œ\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "id": "2Nh1VYeQ5tdt",
    "outputId": "ea0d5e86-a0bd-49f7-b7f4-0721bfedac22",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"so now we have Lama 3 from meta and this model is definitely going to be a GameChanger when it comes to analyzing data with llms here I have L 3 on gr cloud and not only are the text to SQL chains blazing fast they're also capable of generating quite Advanced SQL and almost on par with the high IQ llms if we Implement one additional tweak so in this video I'm going to show you how we can tweak the SQL chains to maximize the performance of Lama 3 we're going to have a look at some of the insights that Lama three is capable of extracting and finally I'm going to briefly discuss some of the implications for llm based data analysis on l.a. comom you can read about Lama 3 and some of the capabilities of the model you can see how the model compares to other popular llms specifically the 70b model that I'm going to be using in this video is compared to Gemini and CLA 3 Sunnet and there's also a link that lets you request access to Lama 3 but this is not what I'll be doing I'm going to be using Gro Cloud because it's the fastest and easiest way to get started using llama 3 and as you can see the 70b model is already available on gr Cloud so I'll just grab the API key and then I'm ready to build the SQL chains with L chain all right the first thing I'm going to do is I'm going to set up the cop notebook and pip install the required libraries then I'll connect to Bay and fetch the schema information from tables in a data set I'll be installing python. EnV to fetch the API Keys Lang chain Gro the Lang chain Gro connector and Google Cloud B quy then I have uploaded myv file with the API keys I have my Google cloud service account key DBQ key. jjon then I have two functions that extracts the schema information from bit query in a schema. py file and this is the same functions that I've been using in the earlier videos that lets me extract and feed the scheme information from bitre to the chain all right so I'm just going to load the environment variables and then I'm going to connect to biy and the data set I'm using is the same data set I used in the last video in the dashboard video it is an e-commerce data set with four tables customers orders products and customer taxs and the two functions in the schema. py file allows me to extract the schema information from the data set as you can see here so now let's connect to Lama 3 on Gro cloud and set set up the SQL chain using Lang chain expression language to connect to Lama 3 on groc cloud we import chat Gro from Lang chain grock and then we instantiate it with a model name in this case I'm using Lama 370b and the prom template will be injecting three things first I'll inject the schema information I'm extracting from the bit crate data set then I'm injecting the main question or the query and finally I'll be injecting a a message history and the message history is The Tweak I mentioned in the beginning so I'm going to have Lama 3 generate SQL code and then I'll use the bitr client to execute that SQL code and if there's an error I'm going to catch the error and feed it back to the chain and this allows me to make the chains self-correcting which is useful when we're dealing with a model that is of a low IQ the SQL chain is assembled in the usual way I'll use a runnable path through to to inject the schema information and to inject the messages of the message history that contains the errors then I use my prompt the language model and a string output passer and this is what we need to generate the SQL code from a prompt now let's move on to generate some insights with this setup I had difficulties having Lama 3 return clean executable SQL so I had to write a function that lets me extract the SQL code from the response and in this extract SQL function I'm simply using regex to extract the SQL from whatever the language model is returning to wrap it all up I'm creating a function that takes a prompt and a number of attempts as input and then it will generate the SQL using the Lang chain SQL chain and try to execute that SQL up to five times and whenever there's an error I'm going to collect the error and feed it back to the chain and try again and in this way the chain will be self-corrected ing because the llm will understand the error message okay so let's try this the first prompt I'm going to give L three is the following give me a list of the best customers including their rank their first name last name and email and the products they purchased and this one it got in the first attempt so the query executed successfully and we can then have a look at the data frame and this is essentially an audience that you could use for marketing purposes you normally create an audience like this in a customer data platform now let's try a different one I'll do a classical one show me the revenue generated in the last 30 days broken down by acquisition Channel and here you can see that the first attempt is unsuccessful it fails but feeding back the error message makes the second attempt successful and here we have Revenue broken down by acquisition source let's do another audience let's say I want the top 100 customers with the highest purchase frequency but with an under average aov and again we see that the first attempt fails and the second attempt is successful and here we only got the names let's say that we want to include the frequency and the aov as well to get the full overview and here we see that the first attempt fails the second attempt also fails but the third attempt is a success and here we have the full audience data frame with the purchase frequency and the average order value so this is very useful so we can use llama 3 for generating insights if we just implement this small tweak of catching the errors and feeding it back to the chain all right so what are the implications of this first of all we now know that with Lama 3 open source llms can be used to generate insights and this is very good news for privacy sensitive use cases so use cases where you want to feed sensitive customer data back to the llm and in real sensitive use cases you probably don't want to use gr Cloud you want to use Lama 3 locally this is also very good news for query heavy applications so text tosql applications can be query heavy if they're rolled out in a big organization so there's a cost consideration that might be worth looking into now Gro cloud is all about realtime gen inference so Lama 3 on gr cloud is going to be really useful for Consumer facing applications where speed is necessary finally I think it's pretty clear now that data pipelines dashboards reports and so on will be llm generated in the future and not so distant future so if you are a data analyst or data engineer you should really pay attention to this and learn this new technology all right that's it for now if you enjoyed this video I suggest you check out one of the other videos on generating SQL with llm chains thanks for watching\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript = docs[0].page_content\n",
    "transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEScwvtSck_C"
   },
   "source": [
    "ì´ì œ, ëŒ€ë³¸ì´ ì£¼ì–´ì§„ YouTube ë¹„ë””ì˜¤ë¥¼ ìš”ì•½í•˜ëŠ” ì²´ì¸ì„ ì„¤ì •í•´ ë³´ê² ìŠµë‹ˆë‹¤. ë¹„ë””ì˜¤ ëŒ€ë³¸ì„ ì…ë ¥ ë³€ìˆ˜ë¡œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì— ì£¼ì…í•œ ë‹¤ìŒ íŒŒì´í”„ ì—°ì‚°ìë¥¼ ì‚¬ìš©í•˜ì—¬ ì²´ì¸ì„ ì„¤ì •í•©ë‹ˆë‹¤. ë¹„ë””ì˜¤ ëŒ€ë³¸ìœ¼ë¡œ í˜¸ì¶œí•˜ë©´ ì›ì‹œ í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì¶”ì¶œí•˜ì§€ ì•Šê³ ë„ ê°„ê²°í•œ ìš”ì•½ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JyEU9cQo7kZF",
    "outputId": "40c8d37c-dffe-40f2-caab-0d0348e7e0e8"
   },
   "outputs": [],
   "source": [
    "# ì „ì‚¬ëœ ë‚´ìš©ì„ chainì— ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "# prompt template ìƒì„±\n",
    "prompt_template = \"\"\"\n",
    "    ë‹¹ì‹ ì€ ìœ íŠœë¸Œ ë¹„ë””ì˜¤ë¥¼ ìš”ì•½í•˜ëŠ” ìœ ìš©í•œ ì¡°ìˆ˜ì…ë‹ˆë‹¤. ë‹¤ìŒ ë¹„ë””ì˜¤ ëŒ€ë³¸ì´ ì£¼ì–´ì§ˆ ë•Œ:\n",
    "    {video_transcript}\n",
    "    í•œêµ­ì–´ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.\n",
    "\"\"\"\n",
    "\n",
    "# prompt instance ìƒì„±\n",
    "prompt = PromptTemplate.from_template(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "MXvV726M8VPN"
   },
   "outputs": [],
   "source": [
    "# chain ìƒì„±\n",
    "chain = prompt | llm\n",
    "\n",
    "answer = chain.invoke({\"video_transcript\": docs}).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OW1qGhFF8e3f",
    "outputId": "d64990be-176c-4715-e60b-4331a8f208bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ ë¹„ë””ì˜¤ëŠ” ë©”íƒ€ì˜ Llama 3 ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ë¶„ì„ ë° SQL ì¿¼ë¦¬ ìƒì„±ì— ëŒ€í•œ ë‚´ìš©ì„ ë‹¤ë£¹ë‹ˆë‹¤. ì£¼ìš” í¬ì¸íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "\n",
      "1. Llama 3ì˜ íŠ¹ì§•:\n",
      "- Grock Cloudì—ì„œ ë¹ ë¥´ê³  ê³ ê¸‰ SQL ìƒì„± ëŠ¥ë ¥\n",
      "- 70b ëª¨ë¸ì€ Gemini, Claude 3ì™€ ë¹„êµ ê°€ëŠ¥í•œ ì„±ëŠ¥\n",
      "\n",
      "2. ë°ì´í„° ë¶„ì„ ë°©ë²•:\n",
      "- ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ\n",
      "- ì˜¤ë¥˜ ë°œìƒ ì‹œ ìë™ ìˆ˜ì • ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„\n",
      "- SQL ì¿¼ë¦¬ ìƒì„± ë° ì‹¤í–‰\n",
      "\n",
      "3. ë°ëª¨ ì¿¼ë¦¬ ì˜ˆì‹œ:\n",
      "- ìµœê³  ê³ ê° ëª©ë¡ ì¶”ì¶œ\n",
      "- ìµœê·¼ 30ì¼ ìˆ˜ìµì„ ì±„ë„ë³„ë¡œ ë¶„ì„\n",
      "- êµ¬ë§¤ ë¹ˆë„ê°€ ë†’ì€ ê³ ê° ì°¾ê¸°\n",
      "\n",
      "4. ì£¼ìš” ì‹œì‚¬ì :\n",
      "- ê°œì¸ì •ë³´ ë³´í˜¸ì— ë¯¼ê°í•œ ì‚¬ìš© ì‚¬ë¡€ì— ì í•©\n",
      "- ì‹¤ì‹œê°„ ë°ì´í„° ë¶„ì„ì— ìœ ìš©\n",
      "- ë¯¸ë˜ì— ë°ì´í„° íŒŒì´í”„ë¼ì¸ê³¼ ëŒ€ì‹œë³´ë“œê°€ LLMìœ¼ë¡œ ìƒì„±ë  ê²ƒ\n",
      "\n",
      "ê²°ë¡ ì ìœ¼ë¡œ, Llama 3ëŠ” ë°ì´í„° ë¶„ì„ì˜ ìƒˆë¡œìš´ ê°€ëŠ¥ì„±ì„ ì œì‹œí•˜ê³  ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Runnableê³¼ LangChain í‘œí˜„ ì–¸ì–´  \n",
    "\n",
    "- LCEL(LangChain Expression Language) ëŠ” Runnables ë¥¼ chain ìœ¼ë¡œ êµ¬ì„±í•˜ëŠ” ë°©ë²•\n",
    "\n",
    "LCELì€ ê¸°ë³¸ êµ¬ì„± ìš”ì†Œì—ì„œ ë³µì¡í•œ ì²´ì¸ì„ êµ¬ì¶•í•˜ëŠ” ê²ƒì„ ê°„ì†Œí™”í•©ë‹ˆë‹¤. íŒŒì´í”„ ì—°ì‚°ì(|)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ êµ¬ì„± ìš”ì†Œë¥¼ ì²´ì¸ìœ¼ë¡œ ì—°ê²°í•˜ê³  í•œ ìš”ì†Œì—ì„œ ë‹¤ìŒ ìš”ì†Œë¡œ ì¶œë ¥ì„ ê³µê¸‰í•©ë‹ˆë‹¤. ì´ëŸ° ë°©ì‹ìœ¼ë¡œ êµ¬ì„±ëœ ì²´ì¸ì˜ ê°„ë‹¨í•œ ì˜ˆë¡œëŠ” ëª¨ë¸ê³¼ ì¶œë ¥ íŒŒì„œê°€ ê²°í•©ëœ í”„ë¡¬í”„íŠ¸ê°€ ìˆìŠµë‹ˆë‹¤.  ì´ëŸ¬í•œ êµ¬ì„± ìš”ì†Œë“¤ì„ runnables ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤.  \n",
    "\n",
    "`chain=prompt | model | output_parser`\n",
    "\n",
    "- Chain êµ¬ì„±  \n",
    "    - ì²´ì¸ì— ëŒ€í•œ ì…ë ¥(ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ì „)\n",
    "    - ì…ë ¥ì€ í”„ë¡¬í”„íŠ¸ë¡œ ì „ì†¡ë©ë‹ˆë‹¤.\n",
    "    - í”„ë¡¬í”„íŠ¸ ê°’ì€ LLM ë˜ëŠ” ì±„íŒ… ëª¨ë¸ë¡œ ì „ì†¡ë©ë‹ˆë‹¤.\n",
    "    - Chatmodelì´ ì±„íŒ… ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    - íŒŒì„œëŠ” ì±„íŒ… ë©”ì‹œì§€ì—ì„œ ë¬¸ìì—´ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "    - ë¬¸ìì—´ì€ ì²´ì¸ì˜ ì¶œë ¥ì…ë‹ˆë‹¤\n",
    "\n",
    "- LangChainì˜ runnable ê°ì²´ë“¤:\n",
    "\n",
    "    - RunnableSequence : ì—¬ëŸ¬ runnable êµ¬ì„± ìš”ì†Œë¥¼ ì—°ê²°í•˜ì—¬ ê° êµ¬ì„± ìš”ì†Œê°€ ì…ë ¥ì„ ì²˜ë¦¬í•˜ê³  ì¶œë ¥ì„ ë‹¤ìŒ êµ¬ì„± ìš”ì†Œì— ì „ë‹¬\n",
    "    - RunnableLambda : Pythonì˜ í˜¸ì¶œ ê°€ëŠ¥í•œ ìš”ì†Œ(í•¨ìˆ˜ ë“±)ë¥¼ ì‹¤í–‰ ê°€ëŠ¥í•œ êµ¬ì„± ìš”ì†Œë¡œ ë°”ê¿”ì„œ ì²´ì¸ìœ¼ë¡œ í†µí•©\n",
    "    - RunnablePassthrough : ì…ë ¥ì„ ë³€ê²½í•˜ì§€ ì•Šê³  í†µê³¼ì‹œí‚¤ê±°ë‚˜ ì¶œë ¥ì— ì¶”ê°€ í‚¤ë¥¼ ì¶”ê°€. placeholder ì—­í• ì„ í•˜ê±°ë‚˜ ì‹œí€€ìŠ¤ì— ìœ ì—°í•˜ê²Œ í†µí•©í•  ìˆ˜ ìˆë‹¤.\n",
    "    - RunnableParallel : ì—¬ëŸ¬ ê°œì˜ ì‹¤í–‰ íŒŒì¼ì„ ë™ì‹œì— ì‹¤í–‰í•˜ì—¬ ë‘ ê°œì˜ ì²´ì¸ì´ ë™ì¼í•œ ì…ë ¥ì—ì„œ ì‹¤í–‰ë˜ì§€ë§Œ ë‹¤ë¥¸ ì¶œë ¥ì„ ë°˜í™˜í•˜ëŠ” ë¶„ê¸°ë¥¼ í—ˆìš©."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runnables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "3s0DaSYO_W0Y"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\n    ë‹¹ì‹ ì€ AI ê°œë…ì„ ìš”ì•½í•˜ëŠ” ìœ ìš©í•œ ì¡°ìˆ˜ì…ë‹ˆë‹¤.\\n    {context}\\n    contextë¥¼ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.\\n')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "summarize_prompt_template = \"\"\"\n",
    "    ë‹¹ì‹ ì€ AI ê°œë…ì„ ìš”ì•½í•˜ëŠ” ìœ ìš©í•œ ì¡°ìˆ˜ì…ë‹ˆë‹¤.\n",
    "    {context}\n",
    "    contextë¥¼ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.\n",
    "\"\"\"\n",
    "\n",
    "summarize_prompt = PromptTemplate.from_template(summarize_prompt_template)\n",
    "summarize_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "InN3aD1qGwmD"
   },
   "source": [
    "###  \"|\" ì—°ì‚°ìë¥¼ ì´ìš©í•˜ì—¬ Runnable Sequence chain ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P0cahXhesl-E",
    "outputId": "eed661f4-9190-4da5-f8df-0139d0df7b64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = summarize_prompt | llm | output_parser\n",
    "\n",
    "print(type(chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "id": "WpJAZmpOsjHA",
    "outputId": "147f9a53-6d84-4d3c-cae6-4322635e23f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainì— ëŒ€í•œ í¬ê´„ì ì¸ ì„¤ëª…ì„ ì œê³µí•˜ê² ìŠµë‹ˆë‹¤:\\n\\nLangChain í•µì‹¬ ê°œë…:\\n1. ì •ì˜\\n- ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ ì˜¤í”ˆì†ŒìŠ¤ í”„ë ˆì„ì›Œí¬\\n- Pythonê³¼ JavaScriptë¡œ êµ¬í˜„ ê°€ëŠ¥\\n\\n2. ì£¼ìš” íŠ¹ì§•\\n- LLM ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ ê°„ì†Œí™”\\n- ë³µì¡í•œ AI ì›Œí¬í”Œë¡œìš° êµ¬í˜„ ì§€ì›\\n- ë‹¤ì–‘í•œ ëª¨ë¸ê³¼ ë„êµ¬ í†µí•©\\n\\n3. ì£¼ìš” êµ¬ì„± ìš”ì†Œ\\n- Prompt ê´€ë¦¬\\n- ë©”ëª¨ë¦¬ ê´€ë¦¬\\n- ë°ì´í„° ìƒ‰ì¸ ë° ê²€ìƒ‰\\n- ì—ì´ì „íŠ¸ ë° ì²´ì¸ ìƒì„±\\n- ì™¸ë¶€ ë„êµ¬ ì—°ê²°\\n\\n4. ì£¼ìš” í™œìš© ì‚¬ë¡€\\n- ëŒ€í™”í˜• AI ì±—ë´‡\\n- ë¬¸ì„œ ë¶„ì„\\n- ë°ì´í„° ìš”ì•½\\n- ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ\\n- ê°œì¸í™”ëœ AI ì• í”Œë¦¬ì¼€ì´ì…˜\\n\\n5. ì¥ì \\n- ìœ ì—°ì„±\\n- í™•ì¥ì„±\\n- ëª¨ë“ˆí˜• ì•„í‚¤í…ì²˜\\n- ë‹¤ì–‘í•œ LLM í†µí•©\\n\\nì´ëŠ” LangChainì˜ í•µì‹¬ ê°œë…ì„ ê°„ëµí•˜ê²Œ ìš”ì•½í•œ ì„¤ëª…ì…ë‹ˆë‹¤.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"context\": \"LangChain ì— ëŒ€í•´ ì„¤ëª…í•´ ì¤˜.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_VLKGZ1HO7d"
   },
   "source": [
    "### RunnableLambda ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oI9MZ0sasxQf",
    "outputId": "c5356d5c-6811-43af-9bfb-31fe88e2638e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n",
      "<class 'langchain_core.runnables.base.RunnableLambda'>\n",
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "# RunnableLambdaë¥¼ ì‚¬ìš©í•˜ì—¬ Python í•¨ìˆ˜ë¥¼ ì²´ì¸ì— ì‚½ì…í•©ë‹ˆë‹¤.\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "summarize_chain = summarize_prompt | llm | output_parser\n",
    "print(type(summarize_chain))\n",
    "\n",
    "# ì‚¬ìš©ì ì •ì˜ ëŒë‹¤ í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ê³  ì´ë¥¼ RunnableLambdaì— ë˜í•‘í•©ë‹ˆë‹¤.\n",
    "length_lambda = RunnableLambda(lambda summary: f\"ìš”ì•½ëœ ê¸¸ì´: {len(summary)} ê¸€ì\")\n",
    "print(type(length_lambda))\n",
    "\n",
    "lambda_chain = summarize_chain | length_lambda\n",
    "print(type(lambda_chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "TAa1W6iVHLnq",
    "outputId": "5b181616-a4f7-49a1-b807-9795fc389b9c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ìš”ì•½ëœ ê¸¸ì´: 325 ê¸€ì'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_chain.invoke({\"context\": \"LangChain ì— ëŒ€í•´ ì„¤ëª…í•´ ì¤˜\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DhIbYeOgIYyi",
    "outputId": "6ee972ca-751b-4b86-a8cd-fefe6fbe8c41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context'] input_types={} partial_variables={} template='\\n    ë‹¹ì‹ ì€ AI ê°œë…ì„ ìš”ì•½í•˜ëŠ” ìœ ìš©í•œ ì¡°ìˆ˜ì…ë‹ˆë‹¤.\\n    {context}\\n    contextë¥¼ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.\\n'\n",
      "client=<openai.resources.chat.completions.Completions object at 0x000001AA4D1F8FD0> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001AA4D202EB0> root_client=<openai.OpenAI object at 0x000001AA4B3EDAF0> root_async_client=<openai.AsyncOpenAI object at 0x000001AA4D1F8FA0> model_name='gpt-4o-mini' model_kwargs={} openai_api_key=SecretStr('**********')\n",
      "\n",
      "RunnableLambda(lambda summary: f'ìš”ì•½ëœ ê¸¸ì´: {len(summary)} ê¸€ì')\n"
     ]
    }
   ],
   "source": [
    "for step in lambda_chain.steps:\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ì²´ì¸ì´ ë‹¬ë¦° ë„êµ¬ ì‚¬ìš©  \n",
    "\n",
    "- ë„êµ¬ëŠ” ì—ì´ì „íŠ¸, ì²´ì¸ ë˜ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ì„¸ìƒê³¼ ìƒí˜¸ ì‘ìš©í•  ìˆ˜ ìˆê²Œ í•˜ëŠ” ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤.\n",
    "    - Python REPL, Wikipdedia, YouTube, Zapier, Gradio, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RkuqKhVpyKxm"
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade -q youtube_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ZF5O2LeB-vD",
    "outputId": "86cbdc7a-8466-410b-864b-7c67acc519db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.youtube.com/watch?v=gKCklJOWB6o&pp=ygUP7L2U65Sp7ZWY64qUIEFJ', 'https://www.youtube.com/watch?v=O5-cNXWWdzs&pp=ygUP7L2U65Sp7ZWY64qUIEFJ']\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import YouTubeSearchTool\n",
    "\n",
    "# YouTubeSearchTool ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "youtube_tool = YouTubeSearchTool()\n",
    "\n",
    "# ì•„ë¬´ í‚¤ì›Œë“œë¡œ ìœ íŠœë¸Œ ê²€ìƒ‰ ì‹¤í–‰\n",
    "results = youtube_tool.run(\"ì½”ë”©í•˜ëŠ” AI\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S5BZ28LTAtul",
    "outputId": "62f913a7-6b6c-415c-d494-4ad127a6a18d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_KNAlnEhJgROgTafqk8jMOIzF', 'function': {'arguments': '{\"query\":\"ì½”ë”©í•˜ëŠ” AI\"}', 'name': 'youtube_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 94, 'total_tokens': 112, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-f3e81846-9c98-4b58-9f7e-9f06d66cc1c2-0', tool_calls=[{'name': 'youtube_search', 'args': {'query': 'ì½”ë”©í•˜ëŠ” AI'}, 'id': 'call_KNAlnEhJgROgTafqk8jMOIzF', 'type': 'tool_call'}], usage_metadata={'input_tokens': 94, 'output_tokens': 18, 'total_tokens': 112, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YouTube ë„êµ¬ë¥¼ LLMì— ë°”ì¸ë”©í•˜ì—¬ parameter ì •ë³´ë¥¼ ì•Œì•„ëƒ„\n",
    "llm_with_tools = llm.bind_tools([youtube_tool])\n",
    "\n",
    "msg = llm_with_tools.invoke(\"ì½”ë”©í•˜ëŠ” AI\")\n",
    "msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jHYOhC0BCPPe",
    "outputId": "7dd07d3c-7316-4d98-849d-5e9e025e67ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'youtube_search',\n",
       "  'args': {'query': 'ì½”ë”©í•˜ëŠ” AI'},\n",
       "  'id': 'call_KNAlnEhJgROgTafqk8jMOIzF',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llm_with_tools ì—ì„œ parameter ì •ë³´ë¥¼ ì•Œì•„ë‚¸ë‹¤\n",
    "msg.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'ì½”ë”©í•˜ëŠ” AI'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg.tool_calls[0][\"args\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S-D_-T27Azsy",
    "outputId": "87aec236-a536-48f7-8af1-bbbc8fd8f656"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001AA4D1F8FD0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001AA4D202EB0>, root_client=<openai.OpenAI object at 0x000001AA4B3EDAF0>, root_async_client=<openai.AsyncOpenAI object at 0x000001AA4D1F8FA0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'youtube_search', 'description': 'search for youtube videos associated with a person. the input to this tool should be a comma separated list, the first part contains a person name and the second a number that is the maximum number of video results to return aka num_results. the second part is optional', 'parameters': {'properties': {'query': {'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}, config={}, config_factories=[])\n",
       "| RunnableLambda(lambda x: x.tool_calls[0]['args'])\n",
       "| YouTubeSearchTool()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llm_with_toolsì—ì„œ ì¶”ì¶œëœ ì¸ìˆ˜ë¡œ \n",
    "# YouTubeSearchToolì„ ì‚¬ìš©í•˜ì—¬ ìœ íŠœë¸Œ ê²€ìƒ‰ ì‹¤í–‰í•œëŠ” chain ìƒì„±\n",
    "chain = llm_with_tools | (lambda x: x.tool_calls[0][\"args\"]) | youtube_tool\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke(\"ì½”ë”©í•˜ëŠ” AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gDYHTp3LIzr4",
    "outputId": "48b727f3-bd82-4602-ebdd-0a257fa76799"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.youtube.com/watch?v=gKCklJOWB6o&pp=ygUP7L2U65Sp7ZWY64qUIEFJ',\n",
       " 'https://www.youtube.com/watch?v=sf52u9ZvKzk&pp=ygUP7L2U65Sp7ZWY64qUIEFJ']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "ast.literal_eval(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chains ì¢…ë¥˜\n",
    "\n",
    "**LangChain**ì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ì²´ì¸(Chain) ë° LLM(ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸) ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.\n",
    "\n",
    "**1. Simple Chain (ë‹¨ì¼ ì²´ì¸)**  \n",
    "- í•˜ë‚˜ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ í†µí•´ LLM(OpenAI)ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.  \n",
    "\n",
    "\n",
    "**2. Simple Sequential Chain (ì—°ì† ì²´ì¸)**  \n",
    "-  ì—¬ëŸ¬ LLM í˜¸ì¶œì„ ì—°ì†ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ì—¬ ì¶œë ¥ì„ ë‹¤ìŒ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.  \n",
    "\n",
    "**3. Document ìš”ì•½ ì²´ì¸**  \n",
    "- **ëª©ì :** í…ìŠ¤íŠ¸ ë¬¸ì„œë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.  \n",
    "\n",
    "**4. í…ìŠ¤íŠ¸ë¥¼ Vector Storeë¡œ ë³€í™˜**  \n",
    "- **4.1 VectortstoreIndexCreator**   \n",
    "- **4.2 Chroma DB ì‚¬ìš©**  \n",
    "\n",
    "**5. HTTP Request Chain (ì›¹ ìš”ì²­ ì²´ì¸)**  \n",
    "- HTTP ìš”ì²­ì„ í†µí•´ ì™¸ë¶€ ì›¹ ë°ì´í„°ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Simple Chain (ë‹¨ì¼ ì²´ì¸)\n",
    "- ê°€ì¥ ê¸°ë³¸ì ì¸ ìœ í˜•ì˜ ì²´ì¸\n",
    "- ì…ë ¥ í”„ë¡¬í”„íŠ¸ë¥¼ ìˆ˜ì‹ í•˜ê³  ì´ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ì—­í• ì„ ë‹´ë‹¹í•˜ëŠ” ì–¸ì–´ ëª¨ë¸(LLM) í•˜ë‚˜ë§Œ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•œêµ­ì—ëŠ” ë‹¤ì–‘í•œ ë§¤ë ¥ì„ ê°€ì§„ ê´€ê´‘ì§€ê°€ ë§ìŠµë‹ˆë‹¤. ë‹¤ìŒì€ í•œêµ­ì—ì„œ ê°€ë³¼ ë§Œí•œ ëª‡ ê°€ì§€ ì¶”ì²œ ì¥ì†Œì…ë‹ˆë‹¤.\n",
      "\n",
      "1. **ì„œìš¸**: í•œêµ­ì˜ ìˆ˜ë„ë¡œ, ê²½ë³µê¶, ì°½ë•ê¶, ëª…ë™, ì¸ì‚¬ë™, ë‚¨ì‚°íƒ€ì›Œ ë“± ë‹¤ì–‘í•œ ì—­ì‚¬ì  ì¥ì†Œì™€ í˜„ëŒ€ì ì¸ ì‡¼í•‘, ë§›ì§‘ì´ ê°€ë“í•©ë‹ˆë‹¤.\n",
      "\n",
      "2. **ë¶€ì‚°**: í•´ìš´ëŒ€, ê´‘ì•ˆë¦¬ í•´ìˆ˜ìš•ì¥, ìê°ˆì¹˜ ì‹œì¥ ë“± ë°”ë‹¤ì™€ í•¨ê»˜ ì¦ê¸¸ ìˆ˜ ìˆëŠ” ë„ì‹œì…ë‹ˆë‹¤. ë˜í•œ íƒœì¢…ëŒ€ì™€ ê°ì²œë¬¸í™”ë§ˆì„ë„ ì¸ê¸° ìˆëŠ” ê´€ê´‘ì§€ì…ë‹ˆë‹¤.\n",
      "\n",
      "3. **ì œì£¼ë„**: ì•„ë¦„ë‹¤ìš´ ìì—° ê²½ê´€ê³¼ ë…íŠ¹í•œ ë¬¸í™”ê°€ ì–´ìš°ëŸ¬ì§„ ê³³ì…ë‹ˆë‹¤. í•œë¼ì‚°, ì„±ì‚°ì¼ì¶œë´‰, ë§Œì¥êµ´, ìš°ë„ ë“± ë‹¤ì–‘í•œ ëª…ì†Œê°€ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "4. **ê²½ì£¼**: ì‹ ë¼ì˜ ê³ ë„ë¡œ, ë¶ˆêµ­ì‚¬ì™€ ì„êµ´ì•”, ê²½ì£¼ ì—­ì‚¬ ìœ ì ì§€êµ¬ ë“± ìœ ë„¤ìŠ¤ì½” ì„¸ê³„ë¬¸í™”ìœ ì‚°ì´ ë§ì•„ ì—­ì‚¬ì™€ ë¬¸í™”ë¥¼ ì²´í—˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "5. **ì „ì£¼**: í•œì˜¥ë§ˆì„ë¡œ ìœ ëª…í•˜ë©°, ì „í†µ ìŒì‹ì¸ ë¹„ë¹”ë°¥ê³¼ ë§‰ê±¸ë¦¬ë¥¼ ë§›ë³¼ ìˆ˜ ìˆëŠ” ê³³ì…ë‹ˆë‹¤. ì „í†µë¬¸í™” ì²´í—˜ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
      "\n",
      "6. **ê°•ë¦‰**: ì•„ë¦„ë‹¤ìš´ í•´ë³€ê³¼ ì»¤í”¼ ê±°ë¦¬ë¡œ ìœ ëª…í•œ ë„ì‹œì…ë‹ˆë‹¤. ê²½í¬ëŒ€ì™€ ì˜¤ì£½í—Œë„ ë°©ë¬¸í•  ë§Œí•©ë‹ˆë‹¤.\n",
      "\n",
      "ì´ ì™¸ì—ë„ í•œêµ­ì—ëŠ” ë‹¤ì–‘í•œ ë§¤ë ¥ì„ ê°€ì§„ ì§€ì—­ì´ ë§ìœ¼ë‹ˆ, ì—¬í–‰ì˜ ëª©ì ì— ë§ì¶° ì„ íƒí•´ ë³´ì„¸ìš”!\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\"{place}ì—ì„œ ê°€ì¥ ê°€ ë³¼ë§Œí•œ ê³³ì€?\")\n",
    "\n",
    "chain = prompt | llm\n",
    "# chain = prompt.pipe(llm)\n",
    "\n",
    "# ì…ë ¥ ë³€ìˆ˜ë§Œ ì§€ì •í•˜ì—¬ ì²´ì¸ì„ ì‹¤í–‰\n",
    "print(chain.invoke(\"í•œêµ­\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Simple Sequential Chains (ì—°ì† ì²´ì¸)\n",
    "- Sequential Chainì€ ì–¸ì–´ ëª¨ë¸ì— ëŒ€í•œ ì¼ë ¨ì˜ ì—°ì† í˜¸ì¶œ í¬í•¨\n",
    "- ì´ ì ‘ê·¼ ë°©ì‹ì€ í•œ í˜¸ì¶œì—ì„œ ìƒì„±ëœ ì¶œë ¥ì„ ë‹¤ë¥¸ í˜¸ì¶œì˜ ì…ë ¥ìœ¼ë¡œ í™œìš©í•  ë•Œ íŠ¹íˆ ìœ ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['place'], input_types={}, partial_variables={}, template='{place}ì—ì„œ ë°©ë¬¸í•˜ê¸° ê°€ì¥ ì¢‹ì€ ì¥ì†Œ 5ê³³ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”\\n\\nì‘ë‹µ:\\n')\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001AA4D1F8FD0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001AA4D202EB0>, root_client=<openai.OpenAI object at 0x000001AA4B3EDAF0>, root_async_client=<openai.AsyncOpenAI object at 0x000001AA4D1F8FA0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| PromptTemplate(input_variables=['budget'], input_types={}, partial_variables={}, template='ì¥ì†Œ ëª©ë¡ì´ ì£¼ì–´ì§€ë©´, ëª¨ë“  ì¥ì†Œë¥¼ ë°©ë¬¸í•˜ëŠ” ë° ë“œëŠ” ë¹„ìš©ê³¼ ë°©ë¬¸ì— í•„ìš”í•œ ë‚ ì§œë¥¼ í˜„ì§€ í†µí™”ë¡œ ì¶”ì‚°í•´ ì£¼ì‹­ì‹œì˜¤. \\nê·¸ë¦¬ê³  ë‚˜ì„œ ì˜ˆì‚° {budget}ê³¼ ë¹„êµí•˜ì—¬ ì¶©ë¶„í•œì§€, ë¶€ì¡±í•œì§€ ê³„ì‚°í•´ ì£¼ì„¸ìš”.\\n\\nì‘ë‹µ:\\n')\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001AA4D1F8FD0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001AA4D202EB0>, root_client=<openai.OpenAI object at 0x000001AA4B3EDAF0>, root_async_client=<openai.AsyncOpenAI object at 0x000001AA4D1F8FA0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_1 = \"\"\"{place}ì—ì„œ ë°©ë¬¸í•˜ê¸° ê°€ì¥ ì¢‹ì€ ì¥ì†Œ 5ê³³ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”\n",
    "\n",
    "ì‘ë‹µ:\n",
    "\"\"\"\n",
    "\n",
    "prompt1 = PromptTemplate.from_template(template_1)\n",
    "\n",
    "chain1 = prompt1 | llm\n",
    "\n",
    "template_2 = \"\"\"ì¥ì†Œ ëª©ë¡ì´ ì£¼ì–´ì§€ë©´, ëª¨ë“  ì¥ì†Œë¥¼ ë°©ë¬¸í•˜ëŠ” ë° ë“œëŠ” ë¹„ìš©ê³¼ ë°©ë¬¸ì— í•„ìš”í•œ ë‚ ì§œë¥¼ í˜„ì§€ í†µí™”ë¡œ ì¶”ì‚°í•´ ì£¼ì‹­ì‹œì˜¤. \n",
    "ê·¸ë¦¬ê³  ë‚˜ì„œ ì˜ˆì‚° {budget}ê³¼ ë¹„êµí•˜ì—¬ ì¶©ë¶„í•œì§€, ë¶€ì¡±í•œì§€ ê³„ì‚°í•´ ì£¼ì„¸ìš”.\n",
    "\n",
    "ì‘ë‹µ:\n",
    "\"\"\"\n",
    "\n",
    "prompt2 = PromptTemplate.from_template(template_2)\n",
    "\n",
    "chain2 = prompt2 | llm\n",
    "\n",
    "final_chain = chain1 | chain2\n",
    "final_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¥ì†Œ ëª©ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ ê° ì¥ì†Œë¥¼ ë°©ë¬¸í•˜ëŠ” ë° ë“œëŠ” ëŒ€ëµì ì¸ ë¹„ìš©ê³¼ í•„ìš”í•œ ë‚ ì§œë¥¼ ì¶”ì‚°í•´ ë³´ê² ìŠµë‹ˆë‹¤. ì•„ë˜ëŠ” í•œêµ­ì—ì„œ ì¶”ì²œí•œ 5ê³³ì˜ ì¥ì†Œì— ëŒ€í•œ ì •ë³´ì…ë‹ˆë‹¤.\n",
      "\n",
      "### 1. ì„œìš¸\n",
      "- **ê²½ë³µê¶**\n",
      "  - **ë¹„ìš©**: ì…ì¥ë£Œ ì•½ 3,000ì› (ì•½ $2.5)\n",
      "  - **ì†Œìš” ì‹œê°„**: 2ì‹œê°„\n",
      "- **í™ëŒ€**\n",
      "  - **ë¹„ìš©**: ì‹ì‚¬ ë° ì¹´í˜ ì•½ 15,000ì› (ì•½ $12)\n",
      "  - **ì†Œìš” ì‹œê°„**: 3ì‹œê°„\n",
      "\n",
      "### 2. ë¶€ì‚°\n",
      "- **í•´ìš´ëŒ€**\n",
      "  - **ë¹„ìš©**: í•´ë³€ ì´ìš© ë¬´ë£Œ, ì‹ì‚¬ ë° ì¹´í˜ ì•½ 20,000ì› (ì•½ $16)\n",
      "  - **ì†Œìš” ì‹œê°„**: 4ì‹œê°„\n",
      "- **ê´‘ì•ˆë¦¬**\n",
      "  - **ë¹„ìš©**: í•´ë³€ ì´ìš© ë¬´ë£Œ, ì•¡í‹°ë¹„í‹° ë¹„ìš© ì•½ 30,000ì› (ì•½ $25)\n",
      "  - **ì†Œìš” ì‹œê°„**: 3ì‹œê°„\n",
      "\n",
      "### 3. ì œì£¼ë„\n",
      "- **í•œë¼ì‚°**\n",
      "  - **ë¹„ìš©**: ì…ì¥ë£Œ ì•½ 2,000ì› (ì•½ $1.5), êµí†µë¹„ í¬í•¨ ì•½ 25,000ì› (ì•½ $20)\n",
      "  - **ì†Œìš” ì‹œê°„**: 6ì‹œê°„\n",
      "- **ì„±ì‚° ì¼ì¶œë´‰**\n",
      "  - **ë¹„ìš©**: ì…ì¥ë£Œ ì•½ 2,000ì› (ì•½ $1.5)\n",
      "  - **ì†Œìš” ì‹œê°„**: 2ì‹œê°„\n",
      "\n",
      "### 4. ê²½ì£¼\n",
      "- **ë¶ˆêµ­ì‚¬**\n",
      "  - **ë¹„ìš©**: ì…ì¥ë£Œ ì•½ 5,000ì› (ì•½ $4)\n",
      "  - **ì†Œìš” ì‹œê°„**: 3ì‹œê°„\n",
      "- **ê²½ì£¼ ì—­ì‚¬ìœ ì ì§€êµ¬**\n",
      "  - **ë¹„ìš©**: ë¬´ë£Œ, êµí†µë¹„ ì•½ 10,000ì› (ì•½ $8)\n",
      "  - **ì†Œìš” ì‹œê°„**: 3ì‹œê°„\n",
      "\n",
      "### 5. ì „ì£¼\n",
      "- **ì „ì£¼ í•œì˜¥ë§ˆì„**\n",
      "  - **ë¹„ìš©**: ë¬´ë£Œ, ì „í†µ ìŒì‹ ì•½ 15,000ì› (ì•½ $12)\n",
      "  - **ì†Œìš” ì‹œê°„**: 4ì‹œê°„\n",
      "\n",
      "### ì´ ë¹„ìš© ë° ì†Œìš” ë‚ ì§œ\n",
      "- **ì´ ë¹„ìš©**: \n",
      "  - ì„œìš¸: 18,000ì› (ì•½ $15)\n",
      "  - ë¶€ì‚°: 50,000ì› (ì•½ $41)\n",
      "  - ì œì£¼ë„: 29,000ì› (ì•½ $24)\n",
      "  - ê²½ì£¼: 15,000ì› (ì•½ $12)\n",
      "  - ì „ì£¼: 15,000ì› (ì•½ $12)\n",
      "\n",
      "**ì´í•©**: 127,000ì› (ì•½ $104)\n",
      "\n",
      "- **ì´ ì†Œìš” ì‹œê°„**: \n",
      "  - ì„œìš¸: 5ì‹œê°„\n",
      "  - ë¶€ì‚°: 7ì‹œê°„\n",
      "  - ì œì£¼ë„: 8ì‹œê°„\n",
      "  - ê²½ì£¼: 6ì‹œê°„\n",
      "  - ì „ì£¼: 4ì‹œê°„\n",
      "\n",
      "**ì´í•©**: 30ì‹œê°„\n",
      "\n",
      "### ê²°ë¡ \n",
      "- **ì´ ë¹„ìš©**: 127,000ì› (ì•½ $104)\n",
      "- **ì´ ì†Œìš” ì‹œê°„**: 30ì‹œê°„\n",
      "\n",
      "ì—¬ê¸°ì— ëŒ€í•œ ì˜ˆì‚°ì´ ì¶©ë¶„í•œì§€ ë¶€ì¡±í•œì§€ëŠ” ê°œì¸ì˜ ì—¬í–‰ ê²½ë¹„ì™€ ê³„íšì— ë”°ë¼ ë‹¤ë¥´ê² ì§€ë§Œ, ëŒ€ì²´ë¡œ í•œêµ­ì˜ ì£¼ìš” ê´€ê´‘ì§€ë¥¼ ë°©ë¬¸í•˜ëŠ” ë° ìˆì–´ í•©ë¦¬ì ì¸ ì˜ˆì‚°ìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. ì¶”ê°€ì ì¸ ê²½ë¹„(ìˆ™ë°•, êµí†µë¹„ ë“±)ë¥¼ ê³ ë ¤í•˜ë©´ ë” ë§ì€ ì˜ˆì‚°ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "review = final_chain.invoke({\"place\": \"í•œêµ­\", \"budget\": \"1,000,000\"})\n",
    "print(review.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
