{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86fc5bb2-017f-434e-8cd6-53ab214a5604",
   "metadata": {
    "id": "86fc5bb2-017f-434e-8cd6-53ab214a5604"
   },
   "source": [
    "# **Retrieval Augmented Generation (RAG) 애플리케이션 구축: Part 2**\n",
    "\n",
    "### 사용자와의 상호작용을 기억하는 메모리 기능과 다단계 검색(multi-step retrieval)을 통합한 RAG 애플리케이션을 구축\n",
    "\n",
    "- **Part 1**: RAG 개념을 소개하고, 최소한의 구현 방법을 설명합니다.  \n",
    "- **Part 2** 기존 구현을 확장하여 대화형 상호작용과 다단계 검색 프로세스를 처리할 수 있도록 개선합니다.\n",
    "\n",
    "여기서는 **과거 대화 기록을 현재 답변 논리에 통합하는 방법**을 중점적으로 다룹니다.  \n",
    "\n",
    "**두 가지 접근 방식**을 다룹니다.\n",
    "\n",
    "1. **체인(Chains)**   - 한 번의 검색 단계만 실행합니다.  \n",
    "\n",
    "2. **에이전트(Agents)**   - LLM이 필요에 따라 **여러 번의 검색 단계 (다중 단계 검색)** 를 수행하도록 자유롭게 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b058beb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a7c2b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith 추적 설정 활성화\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2ef4281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# 사용할 언어 모델의 이름을 지정\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-nano\")\n",
    "# 사용할 임베딩 모델의 이름을 지정\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-3-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05eee8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6ba684-26cf-4860-904e-a4d51380c134",
   "metadata": {
    "id": "fa6ba684-26cf-4860-904e-a4d51380c134"
   },
   "source": [
    "우리가 콘텐츠를 불러올 웹사이트는  RAG Part 1 에서 사용했던 **[Lilian Weng의 \"LLM 기반 자율 에이전트](https://lilianweng.github.io/posts/2023-06-23-agent/)\" 블로그 게시물**이며, 이를 통해 해당 게시물의 내용에 대한 질문에 답 할 수 있도록 할 것입니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef3908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer}\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "assert len(docs) == 1\n",
    "\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\")\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,            \n",
    "    chunk_overlap=200,         \n",
    "    add_start_index=True,      \n",
    ")\n",
    "\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"블로그 글을 {len(all_splits)}개의 하위 문서로 분할했습니다.\")\n",
    "\n",
    "_ = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac23bf1-8f8e-4594-bfb2-6044991d9140",
   "metadata": {},
   "source": [
    "### LangChain 고유 방식 : Part 1 에서 사용했던 상태(state) 기반 접근법 \n",
    "- LangChain의 **RAG 애플리케이션**에서는 사용자 입력, 검색된 문서, 그리고 생성된 답변을 각각 상태(state)라는 구조 안에 별도의 키(key)로 저장했습니다.  \n",
    "- 예를 들면:  \n",
    "   ```python\n",
    "   state = {\n",
    "       \"question\": \"고양이는 몇 살까지 살아요?\",\n",
    "       \"context\": [\"고양이의 평균 수명은 12-18년입니다.\"],\n",
    "       \"answer\": \"고양이는 보통 12-18년 정도 삽니다.\"\n",
    "   }\n",
    "   ```\n",
    "- 하지만 이런 방식은 대화의 흐름을 자연스럽게 표현하기 어려울 수 있으므로 v0.3 부터는 OpenAI 의 메시지 형식을 활용하여 다음과 같은 방식을 권장합니다. \n",
    "\n",
    "---\n",
    "\n",
    "### 새로운 메시지(messages) 기반 시퀀스 접근법 - OpenAI API 방식\n",
    "\n",
    "- 여기에는 다음과 같은 요소들이 포함됩니다:  \n",
    "   1. **사용자 메시지(User Message):** HumanMessage 로 표현\n",
    "   2. **어시스턴트 메시지(Assistant Message):** AIMessage 로 표현  \n",
    "   3. **도구 메시지(Tool Message):** 검색된 문서나 기타 참고 자료 - ToolMessage로 표현\n",
    "   4. **최종 응답**: AIMessage 로 표현\n",
    "      \n",
    "<br>\n",
    "- (참고) OpenAI API 방식:  \n",
    "\n",
    "```python\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"고양이는 몇 살까지 살아요?\"},\n",
    "    {\"role\": \"tool\", \"content\": \"검색 결과: 고양이의 평균 수명은 12-18년입니다.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"고양이는 보통 12-18년 정도 삽니다.\"}\n",
    "]\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d32309-21b4-490c-ba16-0dc835715e25",
   "metadata": {},
   "source": [
    "이 메시지 기반 시퀀스 모델 사용 편의를 위해, LangGraph에서는 내장된 MessagesState 버전을 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f8b22a-df5f-48a6-abbc-56a57f2da7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState, StateGraph\n",
    "\n",
    "graph_builder = StateGraph(MessagesState)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335abbbf-109f-4eca-9563-d2f6c77995d5",
   "metadata": {},
   "source": [
    "**도구 호출(tool-calling)** 을 활용해 검색 단계와 상호작용하는 것에는 또 다른 이점이 있습니다. 바로 **검색에 사용할 쿼리를 모델이 직접 생성할 수 있다는 점**입니다.  \n",
    "\n",
    "이것은 특히 **대화형 환경(conversational setting)** 에서 중요합니다. 사용자의 질문이 **대화 이력(chat history)** 을 기반으로 문맥화되어야 할 수 있기 때문입니다. \n",
    "\n",
    "이 기능 역시 OpenAI API 의 tool calling 기능을 LangChain에서 활용하는 사례입니다.\n",
    "\n",
    "---\n",
    "\n",
    "### **예시 대화**  \n",
    "\n",
    "> **Human:** \"Task Decomposition이란 무엇인가요?\"  \n",
    "> **AI:** \"Task Decomposition은 복잡한 작업을 더 작고 단순한 단계로 나누어, 에이전트나 모델이 더 쉽게 관리할 수 있도록 만드는 과정입니다.\"  \n",
    "> **Human:** \"일반적으로 Task Decomposition을 수행하는 방법에는 어떤 것들이 있나요?\"  \n",
    "\n",
    "- 이 시나리오에서 모델은 `\"Task Decomposition의 일반적인 접근법\"`과 같은 검색 쿼리를 생성할 수 있습니다.  \n",
    "- **도구 호출(tool-calling)** 은 이런 과정을 자연스럽게 지원합니다.  \n",
    "\n",
    "---\n",
    "\n",
    "이제 검색 단계를 **도구(tool)** 로 전환해 보겠습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ce2c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "# 검색(retrieve) 함수를 도구로 정의\n",
    "# - response_format=\"content_and_artifact\": 결과를 콘텐츠(content)와 아티팩트(artifact)로 반환\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str):\n",
    "    \"\"\"\n",
    "    주어진 쿼리와 관련된 정보를 검색합니다. \n",
    "    query: 검색할 텍스트 쿼리입니다. llm 이 생성해 줍니다.\n",
    "    \"\"\"    \n",
    "    # 벡터 스토어에서 유사도 검색 수행. 가장 유사한 두 개의 문서를 반환.\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    \n",
    "    # 검색된 문서를 문자열로 직렬화\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\n\" f\"Content {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    \n",
    "    # 직렬화된 결과(content)와 원본 문서 목록(artifact) 반환.\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a34efb-cbfc-4ce3-a91a-87108c6a4b65",
   "metadata": {},
   "source": [
    "### 체인 방식\n",
    "\n",
    "우리의 그래프는 세 개의 노드로 구성됩니다:\n",
    "\n",
    "1. **사용자 입력을 처리하는 노드** - 검색을 위한 쿼리를 생성하거나, 직접 응답을 생성하는 역할을 합니다.  \n",
    "\n",
    "2. **검색(retrieval) 도구를 실행하는 노드** - 검색 단계를 수행하여 관련 문서를 가져오는 역할을 합니다.  \n",
    "\n",
    "3. **최종 응답을 생성하는 노드** - 검색된 문맥(context)을 활용하여 최종 답변을 생성합니다.  \n",
    "\n",
    "아래에서 이 노드들을 구축하겠습니다.    \n",
    "특히, **LangGraph의 사전 구축된 구성 요소 중 하나인 `ToolNode`를 활용**할 것입니다.    \n",
    "이 `ToolNode`는 도구를 실행하고, 그 결과를 **`ToolMessage` 형태로 상태(state)에 추가**하는 역할을 합니다.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58657340-4334-43af-8744-bbb5dcb3d122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# 단계 1: 도구 호출을 포함할 수 있는 AIMessage를 생성하여 전송합니다.\n",
    "def query_or_respond(state: MessagesState):\n",
    "    \"\"\"검색을 위한 도구 호출을 생성하거나 응답을 생성합니다.\"\"\"\n",
    "    llm_with_tools = llm.bind_tools([retrieve])\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    # MessagesState는 메시지를 덮어쓰지 않고 상태에 추가합니다.\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# 단계 2: 검색을 실행합니다. (검색에 필요한 argument는 llm이 생성)\n",
    "tools = ToolNode([retrieve])\n",
    "\n",
    "# 단계 3: 검색된 내용을 사용하여 응답을 생성합니다. \n",
    "# (검색된 내용을 다시 llm에게 전달)\n",
    "def generate(state: MessagesState):\n",
    "    \"\"\"답변을 생성합니다.\"\"\"\n",
    "    # 최근 생성된 ToolMessages 가져오기\n",
    "    recent_tool_messages = []\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else:\n",
    "            break\n",
    "    tool_messages = recent_tool_messages[::-1]\n",
    "\n",
    "    # 프롬프트 형식으로 변환\n",
    "    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
    "    system_message_content = (\n",
    "        \"당신은 질문-응답 작업을 위한 어시스턴트입니다. \"\n",
    "        \"다음의 검색된 문맥을 사용하여 질문에 답하세요. \"\n",
    "        \"만약 답을 모른다면 모른다고 말하세요. \"\n",
    "        \"최대 세 문장으로 답변하고 간결하게 유지하세요.\"\n",
    "        \"\\n\\n\"\n",
    "        f\"{docs_content}\"\n",
    "    )\n",
    "    conversation_messages = [\n",
    "        message\n",
    "        for message in state[\"messages\"]\n",
    "        if message.type in (\"human\", \"system\")\n",
    "        or (message.type == \"ai\" and not message.tool_calls)\n",
    "    ]\n",
    "    prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
    "\n",
    "    # 실행\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337a9556-7a0e-4fc0-9627-b932471bec09",
   "metadata": {},
   "source": [
    "마지막으로, 우리의 애플리케이션을 단일 그래프 객체로 컴파일합니다. 이 경우, 우리는 단계를 순차적으로 연결합니다. 또한, 첫 번째 `query_or_respond` 단계가 \"쇼트 서킷(어떤 조건이 충족되어 스킵)\" 되어 도구 호출을 생성하지 않는 경우, 사용자에게 직접 응답할 수 있도록 허용합니다. 이를 통해 검색 단계가 필요하지 않은 일반적인 인사와 같은 대화 경험을 지원할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce19b00-c872-4aa7-afe7-23076c7f850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph import MessagesState, StateGraph\n",
    "\n",
    "graph_builder = StateGraph(MessagesState)\n",
    "\n",
    "# 그래프 빌더에 각 노드 추가\n",
    "graph_builder.add_node(query_or_respond)  # 쿼리 생성 또는 응답 노드 추가\n",
    "graph_builder.add_node(tools)  # 검색(도구 호출) 노드 추가\n",
    "graph_builder.add_node(generate)  # 최종 응답 생성 노드 추가\n",
    "\n",
    "# 그래프의 시작점을 설정\n",
    "graph_builder.set_entry_point(\"query_or_respond\")\n",
    "\n",
    "# 조건부 엣지(경로) 설정\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_respond\",  # 첫 번째 노드: query_or_respond\n",
    "    tools_condition,  # 조건 함수: 도구 호출이 필요한지 여부를 판단\n",
    "    {END: END, \"tools\": \"tools\"},  # 조건에 따라 실행 경로 결정\n",
    "    # - END: 도구 호출이 필요하지 않으면 실행 종료\n",
    "    # - \"tools\": 검색이 필요하면 tools 노드로 이동\n",
    ")\n",
    "\n",
    "# 그래프 엣지(경로) 설정\n",
    "graph_builder.add_edge(\"tools\", \"generate\")  # 도구 검색 후 응답 생성 단계로 이동\n",
    "graph_builder.add_edge(\"generate\", END)  # 응답 생성 후 실행 종료\n",
    "\n",
    "# 그래프 컴파일 및 출력\n",
    "graph = graph_builder.compile()\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e936c39-20db-4499-a662-9edfabfd46a8",
   "metadata": {},
   "source": [
    "우리의 애플리케이션을 테스트해 봅시다.  \n",
    "\n",
    "추가적인 검색 단계가 필요하지 않은 메시지에 적절하게 응답하는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ba796f-b5a7-4a04-a921-230dd954f7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_message = \"안녕\"\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e375cedc-427e-4b0c-9d86-64242909532f",
   "metadata": {},
   "source": [
    "그리고 검색을 실행할 때, 쿼리 생성, 검색, 그리고 응답 생성 단계를 스트리밍하여 관찰할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42b97af-7e3c-489a-b876-5efd4611cdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_message = \"Task Decomposition이 뭐지요?\"\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd1fa52-8fc2-4a69-9fb2-de1eb8c682eb",
   "metadata": {},
   "source": [
    "---------------------\n",
    "### 대화 기록의 상태 기반 관리\n",
    "\n",
    "여러 대화 턴과 스레드를 관리하려면, **애플리케이션을 컴파일할 때 체크포인터(checkpointer)를 지정**하기만 하면 됩니다.  \n",
    "LangGraph의 노드들은 상태(state)에 메시지를 계속 추가하는 방식이므로, 호출할 때마다 일관된 대화 기록을 유지할 수 있습니다.  \n",
    "\n",
    "LangGraph에는 **간단한 인메모리 체크포인터(in-memory checkpointer)**가 기본 제공되며, 아래 예제에서는 이를 사용합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a688a41a-4ecd-4621-8595-c00fc4bd949d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48339720-f942-414d-a1cc-6d871ce990a3",
   "metadata": {},
   "source": [
    "이제 이전과 유사하게 호출할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3d0d13-24fb-41ff-bd19-c0c4e6d77064",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_message = \"Task Decomposition이 뭐지요?\"\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c115fd1-8fa7-4bdb-a88d-7f6285c42b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_message = \"그 것의 일반적인 방법을 알려줄 수 있나요?\"\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c6d532-4244-4698-8174-8d8241f2be48",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "모델이 두 번째 질문에서 생성한 쿼리는 대화의 문맥을 포함하고 있다는 점에 주목하세요.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa22f60-b177-4deb-aa34-c4dff5f900d1",
   "metadata": {},
   "source": [
    "### **에이전트(Agents) 방식**  \n",
    "\n",
    "에이전트는 **대형 언어 모델(LLM)의 추론 능력을 활용하여 실행 중에 결정을 내리는 역할**을 합니다.  \n",
    "에이전트를 사용하면 검색 프로세스에서 추가적인 판단을 LLM에게 위임할 수 있습니다.  \n",
    "\n",
    "비록 앞서 설명한 **체인(chain)** 방식보다 동작이 예측 가능하지 않지만,  \n",
    "에이전트는 **하나의 쿼리에 대해 여러 검색 단계를 실행하거나, 단일 검색을 반복적으로 개선할 수 있는 능력**을 갖추고 있습니다.  \n",
    "\n",
    "아래에서는 **최소한의 RAG(Retrieval-Augmented Generation) 에이전트**를 구성합니다.  \n",
    "LangGraph의 **사전 구축된 ReAct 에이전트 생성자**를 사용하면, 이를 단 한 줄의 코드로 구현할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f4e7e7-7602-408c-aeb6-f678d17034b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "agent_executor = create_react_agent(llm, [retrieve], checkpointer=memory)\n",
    "agent_executor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f57c82-29a7-4fd7-98c1-8638e26b448c",
   "metadata": {},
   "source": [
    "이전 구현과의 핵심적인 차이점은, 실행이 최종 생성 단계에서 종료되는 대신, **도구 호출이 다시 원래 LLM 호출로 되돌아간다는 점**입니다.  \n",
    "즉, 모델은 검색된 문맥을 사용하여 질문에 답할 수도 있고, 더 많은 정보를 얻기 위해 **또 다른 도구 호출을 생성할 수도 있습니다**.  \n",
    "\n",
    "이제 이를 테스트해 보겠습니다.  \n",
    "보통 여러 번의 검색 단계가 필요한 질문을 구성하여 확인해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34647d4-07ca-4449-a0f3-e974b580036e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"def234\"}}  \n",
    "\n",
    "input_message = (\n",
    "    \"작업 분해(Task Decomposition)란 무엇인가요?\\n\\n\"\n",
    "    \"답을 찾으면, 해당 방법의 일반적인 실제 수행 방식도 찾아보세요.\"\n",
    ")  \n",
    "# 사용자 입력 메시지 정의. 첫 번째 질문은 작업 분해의 표준 방법을 묻고,  \n",
    "# 이후 해당 방법의 실제 수행 방식까지 조사하도록 요청.\n",
    "for event in agent_executor.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},  \n",
    "    # 사용자 메시지를 포함한 입력을 에이전트 실행자(agent_executor)에 전달\n",
    "    stream_mode=\"values\",  # 스트리밍 모드 설정\n",
    "    config=config,  # 설정 적용\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()  # 마지막 응답 메시지를 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e255152f-8a21-4eb1-9dee-1da2e182e361",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "에이전트가 수행하는 과정은 다음과 같습니다:  \n",
    "\n",
    "1. **작업 분해(Task Decomposition)의 표준 방법을 검색하기 위한 쿼리를 생성합니다.**  \n",
    "2. **첫 번째 답변을 받은 후, 해당 방법의 일반적인 확장을 검색하는 두 번째 쿼리를 생성합니다.**  \n",
    "3. **필요한 모든 문맥을 확보한 후, 최종적으로 질문에 대한 답변을 생성합니다.**  \n",
    "\n",
    "LangSmith 추적을 통해 전체 실행 과정(쿼리 생성 → 검색 → 응답)과 함께 **지연 시간(latency) 및 기타 메타데이터**를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe714b1-5bcc-4104-b5c3-c913c57184bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
