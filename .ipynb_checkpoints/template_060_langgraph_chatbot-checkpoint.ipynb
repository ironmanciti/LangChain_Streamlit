{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "add9da7e",
   "metadata": {
    "id": "U8DZpMrEawNo"
   },
   "source": [
    "# Chatbot 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bd0694",
   "metadata": {
    "id": "7sYz4BNOawNp"
   },
   "source": [
    "LangChain v0.3 부터는 memory 사용을 위해 LangGraph 지속성(LangGraph Persistence)을 사용하는 것이 권장됩니다.\n",
    "\n",
    "이전 버전에서 이미 RunnableWithMessageHistory 또는 BaseChatMessageHistory에 의존하고 있는 코드라면 아무 변경도 필요하지 않습니다. 이 기능은 간단한 채팅 애플리케이션에서 계속 사용할 수 있으며, RunnableWithMessageHistory를 사용하는 코드는 기대한 대로 계속 작동할 것입니다.\n",
    "\n",
    "### 개요 (Overview)\n",
    "LLM을 활용한 챗봇 설계 및 구현 예제를 살펴보겠습니다.\n",
    "이 챗봇은 대화를 나누고 이전 상호작용을 기억할 수 있습니다.\n",
    "\n",
    "이 챗봇은 언어 모델만 사용하여 대화를 진행합니다.\n",
    "다음과 같은 관련 개념도 확인할 수 있습니다:\n",
    "\n",
    "- Conversational RAG: 외부 데이터 소스를 활용한 챗봇\n",
    "- 에이전트(Agents): 액션을 수행할 수 있는 챗봇\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8127fad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbe5d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith 추적 설정 활성화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c434f851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b324fff9",
   "metadata": {
    "id": "7YhZjWjbawNq"
   },
   "source": [
    "먼저 모델을 직접 사용해 봅니다. `ChatModel`은 LangChain의 **\"Runnable\"** 인스턴스이며, 이는 표준화된 인터페이스를 통해 상호작용할 수 있음을 의미합니다.  \n",
    "\n",
    "모델을 간단하게 호출하려면 `.invoke` 메서드에 **메시지 목록**을 전달하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d00aed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba3ba216",
   "metadata": {
    "id": "-98EDJ_mawNr"
   },
   "source": [
    "모델 자체는 **상태(state)** 라는 개념을 가지고 있지 않습니다. 예를 들어, 후속 질문을 하면:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b757052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a877dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fa30ee2",
   "metadata": {
    "id": "I6GsjjivawNs"
   },
   "source": [
    "<br>\n",
    "이제 좋은 응답을 받는 것을 확인할 수 있습니다!  \n",
    "\n",
    "이것이 챗봇이 **대화형 상호작용**을 할 수 있는 기본 아이디어입니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05721c0e",
   "metadata": {
    "id": "UYWFZWBuawNs"
   },
   "source": [
    "## **메시지 지속성 (Message Persistence)**  \n",
    "\n",
    "**LangGraph**는 **내장된 지속성 계층(persistence layer)** 을 구현하여 **여러 번의 대화(turns)** 를 지원하는 챗 애플리케이션에 이상적입니다.  \n",
    "\n",
    "챗 모델을 간단한 LangGraph 애플리케이션으로 감싸면 **메시지 기록을 자동으로 저장(persist)** 할 수 있어, **다중 턴(multi-turn)** 애플리케이션 개발이 훨씬 더 간편해집니다.  \n",
    "\n",
    "LangGraph에는 **간단한 인메모리 체크포인터(in-memory checkpointer)** 가 포함되어 있으며, 아래 예제에서 이를 사용합니다.  \n",
    "\n",
    "### 1) 가장 기본적인 형태의 LLM 그래프 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefa9159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메시지 상태(MessagesState)를 상태 스키마(state_schema)로 사용하여 워크플로우를 정의\n",
    "# MessagesState는 messages 필드 하나만 포함하는 TypedDict 구조\n",
    "# 모델을 호출하는 함수 정의\n",
    "def call_model(state: MessagesState):\n",
    "# 그래프 노드 및 엣지 설정\n",
    "# START 지점에서 \"model\" 노드로 이동하도록 엣지를 추가합니다.\n",
    "# \"model\" 노드에 call_model 함수를 연결합니다.\n",
    "# 메모리 추가 (메시지 기록 저장)\n",
    "# 워크플로우를 메모리 체크포인트와 함께 컴파일합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4d26e5",
   "metadata": {
    "id": "zul9zuiGawNt"
   },
   "source": [
    "이제 매번 `runnable` 객체에 전달할 **`config`** 를 생성해야 합니다.  \n",
    "\n",
    "이 **`config`** 에는 입력에 직접 포함되지는 않지만 유용한 정보가 포함됩니다.  이번 경우, **`thread_id`** 를 포함할 것 입니다.  \n",
    "\n",
    "다음과 같은 형태여야 합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe96ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcafab0c",
   "metadata": {
    "id": "KBNmUJh1awNt"
   },
   "source": [
    "이렇게 하면 하나의 애플리케이션에서 **여러 대화 스레드(thread)**를 지원할 수 있습니다.  \n",
    "\n",
    "여러 사용자가 애플리케이션을 동시에 사용할 때 흔히 필요한 기능입니다.  \n",
    "\n",
    "이제 애플리케이션을 호출할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3069eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 메시지를 리스트 형태로 정의\n",
    "# 이는 대화의 첫 번째 메시지가 됩니다.\n",
    "# 'app.invoke'를 사용하여 애플리케이션을 호출\n",
    "# 첫 번째 매개변수: 입력 메시지\n",
    "# 두 번째 매개변수: 추가적인 설정(config) - 예: thread_id 등\n",
    "# 상태(state)에 있는 모든 메시지 중 마지막 메시지 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b986514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9f910c6",
   "metadata": {
    "id": "pGnh-twbawNt"
   },
   "source": [
    "<br>\n",
    "이제 우리의 챗봇은 우리에 대한 정보를 기억합니다.  \n",
    "\n",
    "`config`에서 다른 **`thread_id`** 를 참조하도록 변경하면, 챗봇이 **새로운 대화**를 시작하는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71398f60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e3d6140",
   "metadata": {
    "id": "6FwatDsDawNt"
   },
   "source": [
    "그러나 우리는 항상 **원래의 대화로 돌아갈 수 있습니다** (데이터베이스에 대화를 **저장(persisting)** 하고 있기 때문입니다)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fc8e33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "842d3c1d",
   "metadata": {
    "id": "ImOJzRsQawNu"
   },
   "source": [
    "<br>\n",
    "이렇게 하면 챗봇이 여러 사용자와 동시에 대화를 할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7319de9",
   "metadata": {
    "id": "IVsbpFCCawNu"
   },
   "source": [
    "지금까지 우리는 모델 주위에 간단한 **지속성 계층(persistence layer, 메모리)** 을 추가한 것에 불과합니다. 이제 **프롬프트 템플릿(prompt template)** 을 추가하여 챗봇을 더 복잡하고 개인화된 형태로 만들어 봅시다.\n",
    "\n",
    "---\n",
    "\n",
    "### 2) 프롬프트 템플릿(Prompt Templates)을 사용하여 LLM 호출 최적화\n",
    "\n",
    "**프롬프트 템플릿** 은 **원시 사용자 입력(raw user input)** 을 LLM이 처리할 수 있는 형식으로 변환하는 데 도움을 줍니다.   \n",
    "\n",
    "1. 먼저, **시스템 메시지(system message)** 를 추가하여 **사용자 정의 지침(custom instructions)** 을 포함시킵니다. (여전히 메시지를 입력으로 사용)  \n",
    "2. 다음으로, 메시지 외에 **더 많은 입력 정보** 를 추가합니다.  \n",
    "\n",
    "#### **시스템 메시지(System Message) 추가하기**\n",
    "\n",
    "시스템 메시지를 추가하기 위해 **`ChatPromptTemplate`** 을 생성합니다. 여기서는 메시지 전달을 위해 **`MessagesPlaceholder`** 를 사용하겠습니다.  \n",
    "\n",
    "이렇게 하면 LLM에 전달되는 입력이 더 구조화되고, 챗봇의 동작을 더 정교하게 제어할 수 있습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd936ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM이 사용자 입력을 더 잘 처리할 수 있도록 프롬프트 템플릿을 설정합니다.\n",
    "        # LLM의 동작 방식을 정의하는 지침\n",
    "        # Messages Placeholder - 이전 대화 메시지들을 전달합"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7ed628",
   "metadata": {
    "id": "9UytXq1UawNu"
   },
   "source": [
    "이제 이 템플릿을 통합하여 애플리케이션을 업데이트할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bc8e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MessagesState를 상태 스키마로 사용하여 워크플로우를 정의합니다.\n",
    "# 모델 호출 함수 정의\n",
    "def call_model(state: MessagesState):\n",
    "# START에서 \"model\" 노드로 이동하도록 엣지 정의\n",
    "# \"model\" 노드에 call_model 함수를 연결\n",
    "# MemorySaver를 사용하여 상태 및 메시지 기록을 저장합니다.\n",
    "# 그래프를 메모리 체크포인트와 함께 컴파일합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e080f7b8",
   "metadata": {
    "id": "J-7IBeVsawNu"
   },
   "source": [
    "같은 방식으로 응용 프로그램을 호출합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf01a1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'configurable' 키를 사용하여 추가적인 설정 값을 전달합니다.\n",
    "# 여기서는 'thread_id'를 사용하여 특정 대화 스레드를 식별합니다.\n",
    "# 메시지 목록에 사용자 메시지 추가\n",
    "# 애플리케이션 호출\n",
    "# 메시지 상태(State), 설정(config) 전달"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eda063d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bd8886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 status의 상태"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dee7b5d",
   "metadata": {
    "id": "KsQjKmlkawNv"
   },
   "source": [
    "내부에서 무슨 일이 일어나고 있는지 이해하는 데 도움이 되도록 [LangSmith 추적](https://smith.langchain.com/o/351c6cd9-1396-5c74-9478-1ee6a22a6433/projects/p/acec9d4d-4978-4597-adff-789cd42e200f?timeModel=%7B%22duration%22%3A%227d%22%7D)을 확인하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa5e76c",
   "metadata": {
    "id": "YPtfr1DrawNz"
   },
   "source": [
    "## **스트리밍 (Streaming)**  \n",
    "\n",
    "이제 우리는 작동하는 챗봇을 만들었습니다. 그러나 챗봇 애플리케이션의 사용자 경험(UX)에서 **매우 중요한 요소** 중 하나는 **스트리밍(Streaming)** 입니다.  \n",
    "\n",
    "LLM은 때때로 응답을 생성하는 데 시간이 걸릴 수 있습니다. 따라서 사용자 경험을 개선하기 위해 대부분의 애플리케이션은 **생성되는 각 토큰을 실시간으로 스트리밍**하여 사용자에게 보여줍니다. 이렇게 하면 사용자는 응답의 진행 상황을 실시간으로 확인할 수 있습니다.  \n",
    "\n",
    "사실, 이 작업은 매우 간단합니다!  \n",
    "\n",
    "기본적으로 LangGraph 애플리케이션에서 `.stream`은 애플리케이션 단계(예: 모델 응답 단계)를 스트리밍합니다. 그러나 `stream_mode=\"messages\"`를 설정하면 **출력 토큰을 실시간으로 스트리밍**할 수 있습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f348b9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스트리밍 설정을 위한 구성(config) 정의\n",
    "# 사용자 입력 메시지 및 언어 설정\n",
    "# 입력 메시지를 HumanMessage 객체로 변환\n",
    "# LangChain 애플리케이션 스트리밍 시작\n",
    "    # AIMessage 객체만 필터링하여 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0b8805",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
