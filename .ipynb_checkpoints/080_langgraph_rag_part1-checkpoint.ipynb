{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5630b0ca",
   "metadata": {
    "id": "5630b0ca"
   },
   "source": [
    "# **Retrieval Augmented Generation (RAG) 애플리케이션 구축: Part 1**\n",
    "\n",
    "### 자신만의 문서를 활용하여 응답을 생성하는 애플리케이션 구축\n",
    "\n",
    "- **Part 1**: RAG 개념을 소개하고, 최소한의 구현 방법을 설명합니다.  \n",
    "- **Part 2** 기존 구현을 확장하여 대화형 상호작용과 다단계 검색 프로세스를 처리할 수 있도록 개선합니다.\n",
    "---\n",
    "\n",
    "## 전형적인 **RAG 애플리케이션**\n",
    "\n",
    "두 가지 주요 구성 요소로 이루어져 있습니다:  \n",
    "\n",
    "1. **인덱싱(Indexing)**: 데이터 소스를 수집하고 인덱싱하는 파이프라인입니다. *일반적으로 오프라인에서 수행됩니다.*  \n",
    "2. **검색 및 생성(Retrieval and Generation)**: 실행 시간에 사용자 쿼리를 받아 인덱스에서 관련 데이터를 검색한 후, 모델에 전달하여 답변을 생성합니다.  \n",
    "\n",
    "---\n",
    "\n",
    "### **인덱싱 단계**  \n",
    "일반적인 데이터 인덱싱 과정은 다음과 같습니다:  \n",
    "\n",
    "1. **로드(Load)**  \n",
    "   - 먼저 데이터를 불러와야 합니다. 이는 문서 로더(Document Loaders)를 사용하여 수행됩니다.  \n",
    "\n",
    "2. **분할(Split)**  \n",
    "   - 텍스트 분할기(Text Splitters)를 사용해 큰 `문서(Document)`를 작은 청크(chunk)로 나눕니다.  \n",
    "   - 이렇게 하면 검색이 더 효율적이며, 모델의 제한된 컨텍스트 윈도우에 맞출 수 있습니다.  \n",
    "\n",
    "3. **저장(Store)**  \n",
    "   - 분할된 데이터를 저장하고 인덱싱할 장소가 필요합니다.  \n",
    "   - 일반적으로 벡터 스토어(VectorStore)와 임베딩 모델(Embeddings)을 사용합니다.  \n",
    "\n",
    "![index_diagram](https://github.com/langchain-ai/langchain/blob/master/docs/static/img/rag_indexing.png?raw=1)\n",
    "\n",
    "---\n",
    "\n",
    "### **검색 및 생성 단계**  \n",
    "일반적인 검색 및 생성 과정은 다음과 같습니다:  \n",
    "\n",
    "4. **검색(Retrieve)**  \n",
    "   - 사용자 입력을 받아 검색기(Retriever)를 사용하여 저장된 데이터에서 관련 청크를 검색합니다.  \n",
    "\n",
    "5. **생성(Generate)**  \n",
    "   - 챗 모델(ChatModel) 또는 LLM이 검색된 데이터를 포함한 프롬프트를 사용해 답변을 생성합니다.  \n",
    "\n",
    "![retrieval_diagram](https://github.com/langchain-ai/langchain/blob/master/docs/static/img/rag_retrieval_generation.png?raw=1)\n",
    "\n",
    "---\n",
    "\n",
    "인덱싱이 완료된 후에는 LangGraph를 오케스트레이션 프레임워크로 사용하여 **검색 및 생성 단계**를 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40f3a87c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8ab6578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith 추적 설정 활성화\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ba74a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# 사용할 언어 모델의 이름을 지정\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-nano\")\n",
    "# 사용할 임베딩 모델의 이름을 지정\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-3-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d92cbc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# InMemoryVectorStore - 메모리 내에서 벡터 데이터를 저장 및 빠른 검색\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466846eb-cf30-447e-85f1-03fc890d9fbf",
   "metadata": {
    "id": "MbLyI5UuawNz"
   },
   "source": [
    "LangSmith를 살펴보면 [LangSmith 추적](https://smith.langchain.com/o/351c6cd9-1396-5c74-9478-1ee6a22a6433/projects/p/acec9d4d-4978-4597-adff-789cd42e200f?timeModel=%7B%22duration%22%3A%227d%22%7D)에서 정확히 무슨 일이 일어나고 있는지 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b2d316-922c-4318-b72d-486fd6813b94",
   "metadata": {
    "id": "93b2d316-922c-4318-b72d-486fd6813b94"
   },
   "source": [
    "이  노트북에서는 **웹사이트 콘텐츠에 대한 질문에 답변하는 애플리케이션**을 구축합니다.  \n",
    "약 **50줄의 코드**로 간단한 **인덱싱 파이프라인**과 **RAG 체인**을 만들 수 있습니다.   \n",
    "**텍스트를 로드, 분할, 인덱싱**한 후, **사용자 질문을 기반으로 관련 데이터를 검색**하고 답변을 생성합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa9ea6a-f914-4f50-8e35-52e6c34b8001",
   "metadata": {
    "id": "efa9ea6a-f914-4f50-8e35-52e6c34b8001"
   },
   "source": [
    "## **단계별 상세 설명**\n",
    "\n",
    "## **1. 인덱싱 (indexing)**\n",
    "\n",
    "### **문서 불러오기 (Loading documents)** \n",
    "우리가 콘텐츠를 불러올 웹사이트는 **[Lilian Weng의 \"LLM 기반 자율 에이전트](https://lilianweng.github.io/posts/2023-06-23-agent/)\" 블로그 게시물**이며, 이를 통해 해당 게시물의 내용에 대한 질문에 답 할 수 있도록 할 것입니다.  \n",
    "먼저 블로그 게시물의 내용을 불러와야 합니다. 이를 위해 **Document Loaders** 중 **[WebBaseLoader](https://python.langchain.com/docs/integrations/document_loaders/web_base/)** 를 사용합니다.  이 객체는 데이터 소스에서 정보를 불러와 **Document** 객체 목록으로 반환합니다.  \n",
    "\n",
    "이 경우,   \n",
    "- `WebBaseLoader`는 `urllib`를 사용해 **웹 URL에서 HTML을 로드**합니다.  \n",
    "- 이후, `BeautifulSoup`을 사용해 **텍스트로 파싱**합니다.  \n",
    "\n",
    "#### **HTML → 텍스트 변환 커스터마이징**  \n",
    "- `bs_kwargs` 매개변수를 사용하여 `BeautifulSoup` 파서에 사용자 정의 옵션을 전달할 수 있습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe124b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 43047\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# HTML에서 `post-content`, `post-title`, `post-header` 클래스를 가진 콘텐츠만 추출하여 텍스트 변환\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "\n",
    "# WebBaseLoader를 사용해 웹 페이지의 내용을 불러옵니다.\n",
    "# Define the headers with a User-Agent\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer}\n",
    ")\n",
    "\n",
    "# 문서 로드하여 Document 객체 목록 반환\n",
    "docs = loader.load()\n",
    "\n",
    "# 로드된 문서가 정확히 하나인지 검증합니다.\n",
    "assert len(docs) == 1\n",
    "\n",
    "# 문서의 총 문자 수 출력\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8643579c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43047\n",
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In\n"
     ]
    }
   ],
   "source": [
    "# 문서 길이\n",
    "print(len(docs[0].page_content))\n",
    "# 첫번째 500 자 출력\n",
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f11795-e19f-4697-bc6e-6d477355a1cd",
   "metadata": {
    "id": "e6f11795-e19f-4697-bc6e-6d477355a1cd"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **문서 분할 (Splitting documents)**  \n",
    "\n",
    "불러온 문서는 **42,000자 이상**으로, 많은 언어 모델의 **컨텍스트 윈도우(context window)** 에 넣기에는 너무 길므로, 너무 긴 입력은 **정보를 효과적으로 찾아내기 어려울 수 있습니다.**  \n",
    "\n",
    "이 문제를 해결하기 위해, **`Document`를 작은 청크(chunk)로 분할**하여 **임베딩(embedding)** 및 **벡터 저장(vector storage)** 에 사용합니다.  \n",
    "이렇게 하면 블로그 게시물의 **가장 관련성 높은 부분만 검색**할 수 있습니다.  \n",
    "\n",
    "---\n",
    "\n",
    "**RecursiveCharacterTextSplitter**는 문서를 **공통 구분자(예: 줄바꿈)** 를  사용해 재귀적으로 분할합니다.  일반적인 텍스트 사용 사례에 가장 적합한 텍스트 분할기입니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6606d8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "블로그 글을 63개의 하위 문서로 분할했습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,            # 각 청크의 최대 문자 수 (1,000자)\n",
    "    chunk_overlap=200,          # 청크 간 겹치는 문자 수 (200자)\n",
    "    add_start_index=True,       # 원본 문서에서 각 청크의 시작 인덱스를 추적\n",
    ")\n",
    "\n",
    "# 불러온 문서를 설정한 기준에 따라 청크로 분할\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# 분할된 청크(서브 문서)의 개수 출력\n",
    "print(f\"블로그 글을 {len(all_splits)}개의 하위 문서로 분할했습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5193e01-6cf1-45b9-9ba5-38caf75162a6",
   "metadata": {
    "id": "f5193e01-6cf1-45b9-9ba5-38caf75162a6"
   },
   "source": [
    "### **문서 저장 (Storing documents)**\n",
    "\n",
    "이제 분할된 **66개의 텍스트 청크**를 인덱싱해야 합니다. 이를 통해 검색할 수 있습니다.  \n",
    "\n",
    "1. 각 **문서 청크**의 내용을 **임베딩(embedding)** 합니다.\n",
    "2. 이 **임베딩을 벡터 스토어(Vector Store)** 에 삽입합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b8da4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abaff535-6123-4523-b644-78af873bccdf', 'bf428fae-3756-41aa-829a-ee2b73757b6e', 'ddf63987-e3b2-4c95-b623-b70dd990f50a']\n"
     ]
    }
   ],
   "source": [
    "# 분할된 문서 청크(all_splits)는 임베딩되어 벡터 스토어에 저장됩니다.\n",
    "# 반환값은 저장된 각 문서 청크의 고유 ID 목록입니다.\n",
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "# 첫 세 개의 문서 ID를 출력합니다.\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57666234-a5b3-4abc-b079-755241bb2b98",
   "metadata": {
    "id": "57666234-a5b3-4abc-b079-755241bb2b98"
   },
   "source": [
    "이로써 **인덱싱(Indexing)** 단계가 완료되었습니다!\n",
    "\n",
    "- 이제 우리는 **질의 가능한 벡터 스토어**를 보유하고 있습니다.  \n",
    "- 블로그 게시물의 청크가 저장되어 있으며, 사용자 질문을 받으면 **관련 청크를 반환**할 수 있습니다.  \n",
    "\n",
    "---\n",
    "\n",
    "## **2. 검색 및 생성 (Retrieval and Generation)**\n",
    "\n",
    "이제 실제 **애플리케이션 로직(application logic)** 을 작성해 보겠습니다.  \n",
    "간단한 애플리케이션을 만들어 다음과 같은 작업을 수행할 것입니다:  \n",
    "\n",
    "1. **사용자 질문을 입력받기**  \n",
    "2. **질문과 관련된 문서 청크 검색**  \n",
    "3. **검색된 문서와 질문을 모델에 전달**  \n",
    "4. **모델이 답변을 생성**  \n",
    "\n",
    "---\n",
    "\n",
    "- 생성 단계에서는 **챗 모델(Chat Model)** 을 사용할 것입니다.  \n",
    "- RAG용 프롬프트는 LangChain의 **[프롬프트 허브(prompt hub)](https://smith.langchain.com/hub/rlm/rag-prompt)** 에 저장되어 있습니다.\n",
    "```\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **프로세스 요약**  \n",
    "1. **사용자 질문 입력 → 문서 검색 → 답변 생성**  \n",
    "2. 관련 문서를 검색하고 질문과 함께 모델에 전달  \n",
    "3. 모델이 **최종 답변**을 생성  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdca94d3-3761-4bfc-9835-c29c49aa26db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "# RAG에 최적화된 프롬프트를 LangChain Hub에서 가져옵니다.\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55ea7dd1-3add-4234-b5d9-8fc2c9392a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.messages[0].prompt.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ee032d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: (question 이 여기 들어갑니다.) \n",
      "Context: (context 내용이 여기 들어갑니다.) \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "# 프롬프트를 사용해 예제 메시지를 생성합니다.\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"(context 내용이 여기 들어갑니다.)\",\n",
    "     \"question\": \"(question 이 여기 들어갑니다.)\"}\n",
    ").to_messages()\n",
    "\n",
    "# 프롬프트로부터 생성된 메시지가 하나인지 확인\n",
    "assert len(example_messages) == 1\n",
    "\n",
    "# 첫 번째 메시지의 내용을 출력\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dfe84d-cc19-4227-bee4-56b69508ab11",
   "metadata": {
    "id": "77dfe84d-cc19-4227-bee4-56b69508ab11"
   },
   "source": [
    "**LangGraph**를 사용하여 **검색(Retrieval)** 과 **생성(Generation)** 단계를 하나의 애플리케이션으로 통합할 것입니다. 이를 통해 다음과 같은 이점을 얻을 수 있습니다:\n",
    "\n",
    "- **다양한 호출 모드 지원:**  \n",
    "  한 번 정의된 애플리케이션 로직은 **스트리밍(streaming)**, **비동기(async)**, **배치 호출(batched calls)** 등 여러 호출 모드를 자동으로 지원합니다.\n",
    "\n",
    "- **간편한 배포:**  \n",
    "  [**LangGraph 플랫폼**](https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/)을 통해 애플리케이션을 더 쉽게 배포할 수 있습니다.\n",
    "\n",
    "- **자동 추적:**  \n",
    "  **LangSmith**가 애플리케이션의 모든 단계를 자동으로 추적합니다.\n",
    "\n",
    "- **확장성:**  \n",
    "  [**데이터 지속성(persistence)**](https://langchain-ai.github.io/langgraph/concepts/persistence/) 및 [**사람의 승인(Human-in-the-loop approval)**](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/)과 같은 핵심 기능을 최소한의 코드 변경으로 손쉽게 추가할 수 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    "## **LangGraph를 사용하기 위한 3가지 핵심 요소**\n",
    "\n",
    "1. **애플리케이션의 상태(State)**  \n",
    "2. **애플리케이션의 노드(Nodes)** - 각 단계의 로직  \n",
    "3. **애플리케이션의 흐름(Control Flow)** - 예) 단계의 순서 및 실행 흐름\n",
    "\n",
    "---\n",
    "\n",
    "### **1. 상태(State)**\n",
    "\n",
    "애플리케이션의 [**상태(state)**](https://langchain-ai.github.io/langgraph/concepts/low_level/#state)는 다음을 관리합니다:  \n",
    "- 애플리케이션에 입력되는 데이터  \n",
    "- 각 단계 간에 전달되는 데이터  \n",
    "- 애플리케이션에서 최종적으로 출력되는 데이터  \n",
    "\n",
    "**상태(State)** 는 일반적으로 `TypedDict`로 정의하지만, 더 정교한 검증과 유연성이 필요하다면 [Pydantic BaseModel](https://langchain-ai.github.io/langgraph/how-tos/state-model/)을 사용할 수 있습니다.\n",
    "- TypedDict: 간단하게 상태의 구조(예: 어떤 데이터가 어떤 형태로 들어와야 하는지)를 정의할 때 사용  \n",
    "- Pydantic BaseModel: 더 복잡한 데이터 검증, 기본값 설정, 사용자 정의 검증 등을 할 때 사용\n",
    "\n",
    "---\n",
    "\n",
    "### **간단한 RAG 애플리케이션 상태 예시**\n",
    "\n",
    "간단한 **RAG 애플리케이션**에서는 다음 항목만 추적하면 됩니다:  \n",
    "1. **입력 질문(question)**  \n",
    "2. **검색된 문맥(retrieved context)**  \n",
    "3. **생성된 답변(generated answer)**  \n",
    "\n",
    "다음 단계에서는 **노드(Nodes)** 를 정의하여 각 애플리케이션 단계를 설정하는 방법을 살펴보겠습니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7bab225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# List: 여러 Document 객체를 리스트로 저장할 때 사용\n",
    "# TypedDict: 상태(State) 객체 정의를 위한 구조체 역할\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# 애플리케이션의 상태(State) 객체 정의\n",
    "class State(TypedDict):\n",
    "    question: str          # 사용자 질문을 저장하는 문자열 필드\n",
    "    context: List[Document]    # 검색된 문서 목록을 저장하는 필드\n",
    "    answer: str           # 생성된 답변을 저장하는 문자열 필드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77868d9a-892f-4b2c-b706-850f96b4464f",
   "metadata": {
    "id": "77868d9a-892f-4b2c-b706-850f96b4464f"
   },
   "source": [
    "#### **노드 (애플리케이션 단계)**\n",
    "\n",
    "간단한 두 단계로 구성된 시퀀스를 정의해 보겠습니다:  \n",
    "\n",
    "1. **검색 (Retrieval)**  \n",
    "2. **생성 (Generation)**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "121cb602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자의 질문을 기반으로 벡터 스토어에서 관련 문서를 검색\n",
    "def retrieve(state: State):\n",
    "    # 벡터 스토어에서 질문과 유사도가 높은 문서를 검색\n",
    "    retrieved_docs = vector_store.similarity_search(state['question'])\n",
    "    return {\"context\": retrieved_docs}\n",
    "    \n",
    "# 검색된 문서와 질문을 기반으로 모델이 답변을 생성\n",
    "def generate(state: State):\n",
    "    # 검색된 문서(context) 내용을 하나의 문자열로 합칩니다.\n",
    "    combined_context = \"\\n\\n\".join(doc.page_content for doc in state['context'])\n",
    "    \n",
    "    # 프롬프트에 질문과 문서 내용을 전달하여 메시지 생성\n",
    "    messages = prompt.invoke({\"context\": combined_context, \"question\": state[\"question\"]})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ac9dc3-d73d-48c3-be05-4b60e0b8bc17",
   "metadata": {
    "id": "d1ac9dc3-d73d-48c3-be05-4b60e0b8bc17"
   },
   "source": [
    "애플리케이션을 하나의 **`graph` 객체**로 컴파일합니다.  여기서는 **검색 단계**와 **생성 단계**를 **단일 시퀀스(sequence)** 로 연결합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61d8eb0f-2045-45b8-b09f-5e9d1c558d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x00000169C7645DC0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "\n",
    "# StateGraph 초기화\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# 검색(retrieve)과 생성(generate) 단계를 순차적으로 실행하도록 설정 (node 자동 추가)\n",
    "workflow.add_sequence([retrieve, generate])\n",
    "\n",
    "# 그래프의 시작점(START)을 'retrieve' 단계와 연결\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "\n",
    "# 그래프를 컴파일하여 최종 그래프 객체를 생성\n",
    "app = workflow.compile()\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "768c7af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Task Decomposition은 복잡한 작업을 더 작은 단계로 나누는 과정을 의미하며, 이를 통해 작업을 더 쉽게 수행하거나 이해할 수 있게 만듭니다. 이는 LLM의 간단한 프롬프트, 작업별 지침, 또는 인간의 입력을 통해 수행될 수 있습니다. 또한, 외부 계획 도구를 활용하는 방법도 포함됩니다.\n"
     ]
    }
   ],
   "source": [
    "# 그래프를 호출하여 사용자 질문에 대한 답변을 생성\n",
    "result = app.invoke({\"question\": \"Task Decomposition이 뭔가요? 한국어로 답해주세요.\"})\n",
    "\n",
    "# 질문\n",
    "#print(f'Question: {result[\"question\"]}\\n\\n')\n",
    "\n",
    "# 문서의 출처\n",
    "#print(f'Context: {result[\"context\"][0].metadata}\\n\\n')\n",
    "\n",
    "# 생성된 답변(Answer)을 출력\n",
    "print(f'Answer: {result[\"answer\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tjC2SdXftfcz",
   "metadata": {
    "id": "tjC2SdXftfcz"
   },
   "source": [
    "-----------------\n",
    "**LangGraph**는 RAG 애플리케이션을 구축하는 데 반드시 필요하지는 않습니다. 실제로 개별 구성 요소를 사용하여 동일한 애플리케이션 로직을 구현할 수도 있습니다. \n",
    "그럼에도 LangGraph를 사용하는 이유는 복잡한 워크플로우 관리, 상태 관리 및 공유, 에이전트 간 협업 강화, 유연성과 확장성 때문 입니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a062bcfe-615b-46cd-a4f4-3b87887ff911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Decomposition이란 복잡한 작업을 더 작고 관리하기 쉬운 단계로 나누는 것을 의미합니다. 이를 통해 문제를 체계적으로 해결하거나 계획을 세울 수 있습니다. 주로 언어 모델이나 인간이 이를 수행하며, 여러 방법이 존재합니다.\n"
     ]
    }
   ],
   "source": [
    "# Langgraph 없이 RAG 구현한 경우\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "question = \"Task Decomposition이 뭔가요? 한국어로 답해주세요.\"\n",
    "\n",
    "# 벡터 스토어에서 질문과 유사한 문서 검색\n",
    "retrieved_docs = vector_store.similarity_search(question)\n",
    "\n",
    "# 검색된 문서들의 내용을 하나의 문자열로 결합\n",
    "docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "# 검색된 문서(Context)와 질문을 프롬프트 템플릿에 전달하여 메시지 생성\n",
    "message = prompt.invoke({\"question\": question, \"context\": docs_content}).to_messages()\n",
    "\n",
    "# LLM(대형 언어 모델)을 호출하여 답변 생성\n",
    "answer = llm.invoke(message)\n",
    "\n",
    "# 생성된 답변 출력\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f7dc4d-cac8-4be9-b44c-df097dc28c81",
   "metadata": {
    "id": "31f7dc4d-cac8-4be9-b44c-df097dc28c81"
   },
   "source": [
    "-----------------------------------------------\n",
    "\n",
    "**LangGraph의 주요 장점은 다음과 같습니다:**  \n",
    "\n",
    "- **다양한 호출 모드 지원:**  \n",
    "  - 출력 토큰을 스트리밍하거나 각 단계의 결과를 스트리밍하려면 위의 로직을 다시 작성해야 합니다.  \n",
    "  - LangGraph는 이러한 호출 모드를 기본적으로 지원합니다.  \n",
    "<p></p>\n",
    "- **자동 추적 및 배포 지원:**  \n",
    "  - **LangSmith**를 통해 애플리케이션 단계를 자동으로 추적합니다.  \n",
    "  - [**LangGraph 플랫폼**](https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/)을 통해 손쉽게 배포할 수 있습니다.  \n",
    "<p></p>\n",
    "- **지속성, 인간 개입(Human-in-the-loop) 지원:**  \n",
    "  - LangGraph는 지속성(persistence) 및 인간 개입(human-in-the-loop) 기능을 기본적으로 지원합니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef88f30-40ca-476b-808d-794cb72d401f",
   "metadata": {
    "id": "4ef88f30-40ca-476b-808d-794cb72d401f"
   },
   "source": [
    "<br>\n",
    "\n",
    "**LangGraph**는 다양한 호출 모드(**동기(sync)**, **비동기(async)**, **스트리밍(streaming)**)를 지원합니다.  \n",
    "\n",
    "###  **스트림 단계(Stream Steps) 반환**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f51436e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'retrieve': {'context': [Document(id='b2146a6f-7f14-4c4a-9e2b-d2ad7ef87437', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578}, page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#'), Document(id='ddf63987-e3b2-4c95-b623-b70dd990f50a', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.'), Document(id='0c0a0713-f9e4-45a0-a3a6-fb79298eb372', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 17352}, page_content='Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\n\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:'), Document(id='e17026f7-7fcc-4072-b0d8-c914f97531b0', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 31986}, page_content='},\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Remaining unclear areas: 2 remaining questions.\\\\nCan you provide more information about how the MVC components are split into separate files?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\\n  }\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.')]}}\n",
      "\n",
      "----------------\n",
      "\n",
      "{'generate': {'answer': 'Task Decomposition이란 복잡한 작업을 더 작고 관리하기 쉬운 하위 단계로 나누는 과정을 말합니다. 이를 통해 모델이나 시스템이 각 단계별로 해결책을 찾거나 계획을 세우기 쉽게 만듭니다. 다양한 방법으로 수행될 수 있으며, 예를 들어 간단한 프롬프트, 작업별 지침, 또는 외부 계획 도구를 활용할 수 있습니다.'}}\n",
      "\n",
      "----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 그래프의 각 단계를 스트리밍 모드로 실행\n",
    "# - stream_mode=\"updates\": 각 단계가 완료될 때마다 업데이트를 반환\n",
    "# - stream_mode=\"values\": 최종 응답값만 반환\n",
    "for step in app.stream(\n",
    "    {\"question\": \"Task Decomposition이 뭔가요? 한국어로 답해주세요.\"}, stream_mode=\"updates\"\n",
    "):\n",
    "    print(f\"{step}\\n\\n----------------\\n\")    # 각 단계의 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff88b8d-2785-4598-83fb-fcf2a22142a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
