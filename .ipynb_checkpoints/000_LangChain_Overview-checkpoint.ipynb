{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TypDOiTPbjE"
   },
   "source": [
    "# LangChain 기능 소개\n",
    "\n",
    "- 1. 프롬프트 템플릿과 로더를 사용하여 체인 구성  \n",
    "- 2. Runnable과 LangChain 표현 언어  \n",
    "- 3. RAG 체인 구축  \n",
    "- 4. 체인이 달린 도구 사용  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOUiSLMxPzZY"
   },
   "source": [
    "## LangChain 개요\n",
    "\n",
    "LangChain으로 개발된 애플리케이션은 일반적으로 세 가지 범주로 나뉩니다.\n",
    "\n",
    "- 챗봇 : LLM을 이용하여 보다 지능적인 마케팅, 고객 지원, 교육 상호작용 구현.\n",
    "- 검색 증강 생성(RAG) Q&A : 대용량 문서 요약, 데이터 분석, 외부 소스를 참조하여 코드 생성에 사용.\n",
    "- 에이전트 시스템은 다중 에이전트 설정과 인간 상호작용을 포함하며 복잡한 워크플로우를 위해 LangGraph를 활용. 공급망 관리 및 운영 최적화와 같은 분야에 적용 가능.\n",
    "\n",
    "LangChain은 자연어를 실행 가능한 프로그램으로 변환하는 파이프라인을 생성할 수 있게 합니다. 이러한 체인을 활용함으로써 사용자는 자연어를 입력하고 보고서, 분석 또는 컴퓨터 프로그램과 같은 출력을 받을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FXRwb1YcsW1j",
    "outputId": "7e5cff59-b6f5-446d-e3cc-93891014f5c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env 파일에서 API 키를 로드\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사용 가능한 LLM 목록 조회"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gpt-4.5-preview', 'gpt-4.5-preview-2025-02-27', 'gpt-4o-mini-audio-preview-2024-12-17', 'dall-e-3', 'dall-e-2', 'gpt-4o-audio-preview-2024-10-01', 'gpt-4o-audio-preview', 'gpt-4o-mini-realtime-preview-2024-12-17', 'gpt-4o-mini-realtime-preview', 'o1-mini-2024-09-12', 'o1-mini', 'omni-moderation-latest', 'gpt-4o-mini-audio-preview', 'omni-moderation-2024-09-26', 'whisper-1', 'gpt-4o-realtime-preview-2024-10-01', 'babbage-002', 'gpt-4-turbo-preview', 'chatgpt-4o-latest', 'tts-1-hd-1106', 'text-embedding-3-large', 'gpt-4-0125-preview', 'gpt-4o-audio-preview-2024-12-17', 'gpt-4', 'gpt-4o-2024-05-13', 'tts-1-hd', 'o1-preview', 'o1-preview-2024-09-12', 'gpt-4o-2024-11-20', 'gpt-3.5-turbo-instruct-0914', 'gpt-4o-mini-2024-07-18', 'gpt-4o-mini', 'tts-1', 'tts-1-1106', 'davinci-002', 'gpt-3.5-turbo-1106', 'gpt-4-turbo', 'gpt-3.5-turbo-instruct', 'o1', 'gpt-4o-2024-08-06', 'gpt-3.5-turbo-0125', 'gpt-4o-realtime-preview-2024-12-17', 'gpt-3.5-turbo', 'gpt-4-turbo-2024-04-09', 'gpt-4o-realtime-preview', 'gpt-3.5-turbo-16k', 'gpt-4o', 'text-embedding-3-small', 'gpt-4-1106-preview', 'text-embedding-ada-002', 'o3-mini-2025-01-31', 'gpt-4-0613', 'o3-mini', 'o1-2024-12-17']\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# 사용 가능한 모델 목록을 가져옵니다.\n",
    "models = openai.models.list()\n",
    "\n",
    "# 각 모델의 ID 출력\n",
    "print([model.id for model in models])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEi1gjmpW8ab"
   },
   "source": [
    "## LLM 연결\n",
    "\n",
    "OpenAI 및 Anthropic API에 연결합니다. 원하는 모델을 지정하여 ChatOpenAI 및 ChatAnthropic 클래스의 인스턴스를 만듭니다. llm_claude3 인스턴스를 사용하여 간단한 쿼리를 호출하여 설정을 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jSY2uUq8ssCp"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# from langchain_anthropic import ChatAnthropic\n",
    "# llm = ChatAnthropic(model=\"claude-3-5-haiku-20241022\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "id": "NFWMvMRitGX-",
    "outputId": "9eff0799-8467-4610-ee63-d3b945657364"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain은 대규모 언어 모델(LLM)을 활용한 애플리케이션 개발을 간소화하는 오픈 소스 Python 프레임워크입니다. 주요 특징과 기능은 다음과 같습니다:\n",
      "\n",
      "1. 주요 목적\n",
      "- AI 애플리케이션 개발 촉진\n",
      "- LLM 기반 애플리케이션 구축 복잡성 감소\n",
      "- 다양한 AI 작업 지원\n",
      "\n",
      "2. 핵심 구성 요소\n",
      "- Prompt 관리\n",
      "- 메모리 기능\n",
      "- 데이터 색인\n",
      "- 체인(Chain) 생성\n",
      "- 에이전트 개발\n",
      "- 외부 도구 통합\n",
      "\n",
      "3. 주요 기능\n",
      "- 문서 로딩 및 처리\n",
      "- 텍스트 분할\n",
      "- 벡터 저장소 연결\n",
      "- API 통합\n",
      "- 대화형 인터페이스 구현\n",
      "\n",
      "4. 지원하는 주요 모델\n",
      "- OpenAI GPT\n",
      "- Hugging Face 모델\n",
      "- Google Vertex AI\n",
      "- Anthropic Claude\n",
      "\n",
      "5. 활용 사례\n",
      "- 챗봇\n",
      "- 문서 분석\n",
      "- 질의응답 시스템\n",
      "- 데이터 요약\n",
      "- 코드 생성\n",
      "\n",
      "LangChain은 AI 개발자들에게 강력하고 유연한 개발 도구를 제공합니다.\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"한국어로 LangChain 에 대해서 설명해줘\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Messages\n",
    "- AIMessage: AI(인공지능) 모델이 생성한 메시지를 나타냅니다.  \n",
    "예를 들어, AI 모델이 사용자의 질문에 대해 응답을 생성할 때 이 클래스가 사용됩니다.\n",
    "\n",
    "- HumanMessage: 사람(사용자)이 생성한 메시지를 나타냅니다.  \n",
    "사용자가 입력한 텍스트나 질문 등이 여기에 해당됩니다.\n",
    "\n",
    "- StemMessage: AI 행동을 프라이밍하기 위한 메시지.\n",
    "시스템 메시지는 일반적으로 일련의 입력 메시지 중 첫 번째로 전달됩니다. 일반적으로 대화의 흐름을 제어하거나 특정 지침을 제공하기 위해 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "마치 다섯살 먹은 어린아이에게 설명하듯이 한국어로 쉽게 설명해 주세요.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "LangChain이 무엇인가요?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gYhhibXZSGTF",
    "outputId": "3d276aa8-39a2-4b85-b9aa-5bb43729783b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='어린 아이에게 설명하듯이 LangChain을 알려드릴게요! 🤖\\n\\nLangChain은 마치 레고 블록 같은 멋진 프로그램이에요. 이 특별한 레고 블록들은 인공지능(AI)과 대화하고 멋진 일들을 할 수 있게 도와줍니다.\\n\\n예를 들어:\\n• 챗GPT 같은 AI를 더 똑똑하게 만들어요\\n• 복잡한 질문에 답할 수 있게 해줘요\\n• 여러 AI 도구들을 쉽게 연결할 수 있어요\\n\\n마치 레고로 원하는 모양을 만들 듯이, LangChain으로 개발자들은 AI로 원하는 기능을 만들 수 있어요. \\n\\n간단하게 말해, LangChain은 AI와 놀 수 있는 멋진 놀이도구라고 생각하면 돼요! 🌟', additional_kwargs={}, response_metadata={'id': 'msg_01LDbx9Sx2w3suk77GBcsdHe', 'model': 'claude-3-5-haiku-20241022', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 66, 'output_tokens': 312}}, id='run-178266a3-6ba1-4525-9c69-855cd68eee1d-0', usage_metadata={'input_tokens': 66, 'output_tokens': 312, 'total_tokens': 378, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# system과 human/user 메시지를 이용한 기본 요청\n",
    "\n",
    "message = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=user_prompt),\n",
    "]\n",
    "\n",
    "response = llm.invoke(message)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "AK7Xe8wnSQGa",
    "outputId": "c314481f-1ecb-4337-bf3c-08c274701b7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어린 아이에게 설명하듯이 LangChain을 알려드릴게요! 🤖\n",
      "\n",
      "LangChain은 마치 레고 블록 같은 멋진 프로그램이에요. 이 특별한 레고 블록들은 인공지능(AI)과 대화하고 멋진 일들을 할 수 있게 도와줍니다.\n",
      "\n",
      "예를 들어:\n",
      "• 챗GPT 같은 AI를 더 똑똑하게 만들어요\n",
      "• 복잡한 질문에 답할 수 있게 해줘요\n",
      "• 여러 AI 도구들을 쉽게 연결할 수 있어요\n",
      "\n",
      "마치 레고로 원하는 모양을 만들 듯이, LangChain으로 개발자들은 AI로 원하는 기능을 만들 수 있어요. \n",
      "\n",
      "간단하게 말해, LangChain은 AI와 놀 수 있는 멋진 놀이도구라고 생각하면 돼요! 🌟\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 수학 선생님"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81을 9로 나누면 9입니다.\n",
      "\n",
      "계산 과정:\n",
      "81 ÷ 9 = 9\n",
      "\n",
      "확인 방법:\n",
      "9 × 9 = 81 이므로 정답은 9입니다.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"당신은  수학 문제를 잘 풀어주는 도움되는 assistant 입니다.\"),\n",
    "    HumanMessage(content=\"81을 9로 나누면 몇인가요?\")\n",
    "]\n",
    "\n",
    "result = llm.invoke(messages)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 곱하기 5는 50입니다.\n",
      "\n",
      "계산 과정:\n",
      "10 × 5 = 50\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"당신은  수학 문제를 잘 풀어주는 도움되는 assistant 입니다.\"),\n",
    "    HumanMessage(content=\"81을 9로 나누면 몇인가요?\"),\n",
    "    AIMessage(content=\"81을 9로 나누면 9입니다.\"),\n",
    "    HumanMessage(content=\"10 곱하기 5는 얼마인가요?\")\n",
    "]\n",
    "\n",
    "result = llm.invoke(messages)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fi6_NvW3MaE"
   },
   "source": [
    "##  1. 프롬프트 템플릿과 로더를 사용하여 체인 구성  \n",
    "\n",
    "- LLM에서의 Chain 은 Data Processing 에서의 Pipeline 과 유사한 개념"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G447ezl5X5ye"
   },
   "source": [
    "### 체인의 구성 요소 - Runnables\n",
    "- 프롬프트 : LLM의 응답을 안내하는 템플릿  \n",
    "- LLM 또는 채팅 모델 : 프롬프트에 따라 응답을 생성하는 엔진  \n",
    "- 출력 파서 : LLM의 출력을 파싱하는 도구  \n",
    "- 도구 : LLM이 API에서 추가 정보를 추출하거나 코드를 실행하여 LLM을 에이전트로 전환할 수 있게 해주는 확장 기능  \n",
    "- 일반 함수 : 서로 연결될 수 있는 추가적인 일반 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Yt1j9kXR1h53"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='\\n    당신은 AI 주제를 설명하는 데 도움이 되는 어시스턴트입니다. 다음 입력이 주어지면:\\n    {topic}\\n    주어진 주제에 대한 설명을 제공하세요.\\n')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# simple prompt template 생성\n",
    "# {topic} 변수에서 사용자의 query 가 대체된다.\n",
    "prompt_template = \"\"\"\n",
    "    당신은 AI 주제를 설명하는 데 도움이 되는 어시스턴트입니다. 다음 입력이 주어지면:\n",
    "    {topic}\n",
    "    주어진 주제에 대한 설명을 제공하세요.\n",
    "\"\"\"\n",
    "\n",
    "# prompt template 을 이용하여 prompt 생성 - old style\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "# prompt template 을 이용하여 prompt 생성 - new style\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "id": "0vikrfE53yDk",
    "outputId": "adf40012-9025-4a1a-a591-da9fd5b4883c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain은 대규모 언어 모델(LLM)을 사용하여 더 복잡하고 유용한 애플리케이션을 개발하기 위한 오픈 소스 Python 프레임워크입니다. 주요 특징은 다음과 같습니다:\n",
      "\n",
      "1. 주요 기능\n",
      "- LLM과 상호작용을 쉽게 만드는 도구 제공\n",
      "- 다양한 AI 애플리케이션 개발 지원\n",
      "- 프롬프트 관리, 메모리, 체인 등의 기능 포함\n",
      "\n",
      "2. 주요 사용 사례\n",
      "- 챗봇 개발\n",
      "- 문서 분석\n",
      "- 데이터 요약\n",
      "- 질문-답변 시스템\n",
      "- AI 에이전트 생성\n",
      "\n",
      "3. 주요 장점\n",
      "- 유연성\n",
      "- 쉬운 통합\n",
      "- 다양한 AI 모델 지원\n",
      "- 복잡한 AI 워크플로우 구현 용이\n",
      "\n",
      "개발자들이 AI 애플리케이션을 더 효율적으로 구축할 수 있게 해주는 강력한 프레임워크입니다.\n"
     ]
    }
   ],
   "source": [
    "# pipe operator \"|\"\" 를 이용하여 하나 이상의 chain 을 결합\n",
    "chain = prompt | llm\n",
    "\n",
    "print(chain.invoke({\"topic\": \"LangChain이 뭐예요?\"}).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJzXNZCnZRCr"
   },
   "source": [
    "### document loader 를 이용하여 script 요약\n",
    "더 고급 체인을 만들기 위해 LangChain을 사용하여 YouTube 비디오를 필사합니다. 먼저 YouTube Transcript API를 설치합니다. 그런 다음 LangChain의 커뮤니티 문서 로더에서 YouTube Loader를 가져옵니다. 로더에 비디오 URL을 전달하면 필사본을 추출할 수 있으며, 이는 문서 목록으로 반환됩니다. 원시 필사본을 얻으려면 이러한 문서에서 페이지 콘텐츠를 추출하거나 문서를 체인에 직접 전달하기만 하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "L1HMGE6k4mQp"
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade --quiet youtube-transcript-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "3r04ZHou46-F"
   },
   "outputs": [],
   "source": [
    "# LangChain 커뮤니티에서 제공하는 Youtube Loader를 임포트\n",
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "# YoutubeLoader를 사용하여 유튜브 URL에서 로더를 생성\n",
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=AOEGOhkGtjI\", add_video_info=False\n",
    ")\n",
    "\n",
    "# 비디오의 자막을 문서로 로드\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "id": "2Nh1VYeQ5tdt",
    "outputId": "ea0d5e86-a0bd-49f7-b7f4-0721bfedac22",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"so now we have Lama 3 from meta and this model is definitely going to be a GameChanger when it comes to analyzing data with llms here I have L 3 on gr cloud and not only are the text to SQL chains blazing fast they're also capable of generating quite Advanced SQL and almost on par with the high IQ llms if we Implement one additional tweak so in this video I'm going to show you how we can tweak the SQL chains to maximize the performance of Lama 3 we're going to have a look at some of the insights that Lama three is capable of extracting and finally I'm going to briefly discuss some of the implications for llm based data analysis on l.a. comom you can read about Lama 3 and some of the capabilities of the model you can see how the model compares to other popular llms specifically the 70b model that I'm going to be using in this video is compared to Gemini and CLA 3 Sunnet and there's also a link that lets you request access to Lama 3 but this is not what I'll be doing I'm going to be using Gro Cloud because it's the fastest and easiest way to get started using llama 3 and as you can see the 70b model is already available on gr Cloud so I'll just grab the API key and then I'm ready to build the SQL chains with L chain all right the first thing I'm going to do is I'm going to set up the cop notebook and pip install the required libraries then I'll connect to Bay and fetch the schema information from tables in a data set I'll be installing python. EnV to fetch the API Keys Lang chain Gro the Lang chain Gro connector and Google Cloud B quy then I have uploaded myv file with the API keys I have my Google cloud service account key DBQ key. jjon then I have two functions that extracts the schema information from bit query in a schema. py file and this is the same functions that I've been using in the earlier videos that lets me extract and feed the scheme information from bitre to the chain all right so I'm just going to load the environment variables and then I'm going to connect to biy and the data set I'm using is the same data set I used in the last video in the dashboard video it is an e-commerce data set with four tables customers orders products and customer taxs and the two functions in the schema. py file allows me to extract the schema information from the data set as you can see here so now let's connect to Lama 3 on Gro cloud and set set up the SQL chain using Lang chain expression language to connect to Lama 3 on groc cloud we import chat Gro from Lang chain grock and then we instantiate it with a model name in this case I'm using Lama 370b and the prom template will be injecting three things first I'll inject the schema information I'm extracting from the bit crate data set then I'm injecting the main question or the query and finally I'll be injecting a a message history and the message history is The Tweak I mentioned in the beginning so I'm going to have Lama 3 generate SQL code and then I'll use the bitr client to execute that SQL code and if there's an error I'm going to catch the error and feed it back to the chain and this allows me to make the chains self-correcting which is useful when we're dealing with a model that is of a low IQ the SQL chain is assembled in the usual way I'll use a runnable path through to to inject the schema information and to inject the messages of the message history that contains the errors then I use my prompt the language model and a string output passer and this is what we need to generate the SQL code from a prompt now let's move on to generate some insights with this setup I had difficulties having Lama 3 return clean executable SQL so I had to write a function that lets me extract the SQL code from the response and in this extract SQL function I'm simply using regex to extract the SQL from whatever the language model is returning to wrap it all up I'm creating a function that takes a prompt and a number of attempts as input and then it will generate the SQL using the Lang chain SQL chain and try to execute that SQL up to five times and whenever there's an error I'm going to collect the error and feed it back to the chain and try again and in this way the chain will be self-corrected ing because the llm will understand the error message okay so let's try this the first prompt I'm going to give L three is the following give me a list of the best customers including their rank their first name last name and email and the products they purchased and this one it got in the first attempt so the query executed successfully and we can then have a look at the data frame and this is essentially an audience that you could use for marketing purposes you normally create an audience like this in a customer data platform now let's try a different one I'll do a classical one show me the revenue generated in the last 30 days broken down by acquisition Channel and here you can see that the first attempt is unsuccessful it fails but feeding back the error message makes the second attempt successful and here we have Revenue broken down by acquisition source let's do another audience let's say I want the top 100 customers with the highest purchase frequency but with an under average aov and again we see that the first attempt fails and the second attempt is successful and here we only got the names let's say that we want to include the frequency and the aov as well to get the full overview and here we see that the first attempt fails the second attempt also fails but the third attempt is a success and here we have the full audience data frame with the purchase frequency and the average order value so this is very useful so we can use llama 3 for generating insights if we just implement this small tweak of catching the errors and feeding it back to the chain all right so what are the implications of this first of all we now know that with Lama 3 open source llms can be used to generate insights and this is very good news for privacy sensitive use cases so use cases where you want to feed sensitive customer data back to the llm and in real sensitive use cases you probably don't want to use gr Cloud you want to use Lama 3 locally this is also very good news for query heavy applications so text tosql applications can be query heavy if they're rolled out in a big organization so there's a cost consideration that might be worth looking into now Gro cloud is all about realtime gen inference so Lama 3 on gr cloud is going to be really useful for Consumer facing applications where speed is necessary finally I think it's pretty clear now that data pipelines dashboards reports and so on will be llm generated in the future and not so distant future so if you are a data analyst or data engineer you should really pay attention to this and learn this new technology all right that's it for now if you enjoyed this video I suggest you check out one of the other videos on generating SQL with llm chains thanks for watching\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript = docs[0].page_content\n",
    "transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEScwvtSck_C"
   },
   "source": [
    "이제, 대본이 주어진 YouTube 비디오를 요약하는 체인을 설정해 보겠습니다. 비디오 대본을 입력 변수로 프롬프트 템플릿에 주입한 다음 파이프 연산자를 사용하여 체인을 설정합니다. 비디오 대본으로 호출하면 원시 텍스트를 수동으로 추출하지 않고도 간결한 요약을 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JyEU9cQo7kZF",
    "outputId": "40c8d37c-dffe-40f2-caab-0d0348e7e0e8"
   },
   "outputs": [],
   "source": [
    "# 전사된 내용을 chain에 사용합니다.\n",
    "# prompt template 생성\n",
    "prompt_template = \"\"\"\n",
    "    당신은 유튜브 비디오를 요약하는 유용한 조수입니다. 다음 비디오 대본이 주어질 때:\n",
    "    {video_transcript}\n",
    "    한국어로 요약해 주세요.\n",
    "\"\"\"\n",
    "\n",
    "# prompt instance 생성\n",
    "prompt = PromptTemplate.from_template(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "MXvV726M8VPN"
   },
   "outputs": [],
   "source": [
    "# chain 생성\n",
    "chain = prompt | llm\n",
    "\n",
    "answer = chain.invoke({\"video_transcript\": docs}).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OW1qGhFF8e3f",
    "outputId": "d64990be-176c-4715-e60b-4331a8f208bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 비디오는 메타의 Llama 3 대규모 언어 모델(LLM)을 사용하여 데이터 분석 및 SQL 쿼리 생성에 대한 내용을 다룹니다. 주요 포인트는 다음과 같습니다:\n",
      "\n",
      "1. Llama 3의 특징:\n",
      "- Grock Cloud에서 빠르고 고급 SQL 생성 능력\n",
      "- 70b 모델은 Gemini, Claude 3와 비교 가능한 성능\n",
      "\n",
      "2. 데이터 분석 방법:\n",
      "- 스키마 정보 추출\n",
      "- 오류 발생 시 자동 수정 메커니즘 구현\n",
      "- SQL 쿼리 생성 및 실행\n",
      "\n",
      "3. 데모 쿼리 예시:\n",
      "- 최고 고객 목록 추출\n",
      "- 최근 30일 수익을 채널별로 분석\n",
      "- 구매 빈도가 높은 고객 찾기\n",
      "\n",
      "4. 주요 시사점:\n",
      "- 개인정보 보호에 민감한 사용 사례에 적합\n",
      "- 실시간 데이터 분석에 유용\n",
      "- 미래에 데이터 파이프라인과 대시보드가 LLM으로 생성될 것\n",
      "\n",
      "결론적으로, Llama 3는 데이터 분석의 새로운 가능성을 제시하고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Runnable과 LangChain 표현 언어  \n",
    "\n",
    "- LCEL(LangChain Expression Language) 는 Runnables 를 chain 으로 구성하는 방법\n",
    "\n",
    "LCEL은 기본 구성 요소에서 복잡한 체인을 구축하는 것을 간소화합니다. 파이프 연산자(|)를 사용하여 다양한 구성 요소를 체인으로 연결하고 한 요소에서 다음 요소로 출력을 공급합니다. 이런 방식으로 구성된 체인의 간단한 예로는 모델과 출력 파서가 결합된 프롬프트가 있습니다.  이러한 구성 요소들을 runnables 라고 부릅니다.  \n",
    "\n",
    "`chain=prompt | model | output_parser`\n",
    "\n",
    "- Chain 구성  \n",
    "    - 체인에 대한 입력(일반적으로 사전)\n",
    "    - 입력은 프롬프트로 전송됩니다.\n",
    "    - 프롬프트 값은 LLM 또는 채팅 모델로 전송됩니다.\n",
    "    - Chatmodel이 채팅 메시지를 반환합니다.\n",
    "    - 파서는 채팅 메시지에서 문자열을 추출합니다.\n",
    "    - 문자열은 체인의 출력입니다\n",
    "\n",
    "- LangChain의 runnable 객체들:\n",
    "\n",
    "    - RunnableSequence : 여러 runnable 구성 요소를 연결하여 각 구성 요소가 입력을 처리하고 출력을 다음 구성 요소에 전달\n",
    "    - RunnableLambda : Python의 호출 가능한 요소(함수 등)를 실행 가능한 구성 요소로 바꿔서 체인으로 통합\n",
    "    - RunnablePassthrough : 입력을 변경하지 않고 통과시키거나 출력에 추가 키를 추가. placeholder 역할을 하거나 시퀀스에 유연하게 통합할 수 있다.\n",
    "    - RunnableParallel : 여러 개의 실행 파일을 동시에 실행하여 두 개의 체인이 동일한 입력에서 실행되지만 다른 출력을 반환하는 분기를 허용."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runnables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "3s0DaSYO_W0Y"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\n    당신은 AI 개념을 요약하는 유용한 조수입니다.\\n    {context}\\n    context를 한국어로 요약해 주세요.\\n')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "summarize_prompt_template = \"\"\"\n",
    "    당신은 AI 개념을 요약하는 유용한 조수입니다.\n",
    "    {context}\n",
    "    context를 한국어로 요약해 주세요.\n",
    "\"\"\"\n",
    "\n",
    "summarize_prompt = PromptTemplate.from_template(summarize_prompt_template)\n",
    "summarize_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "InN3aD1qGwmD"
   },
   "source": [
    "###  \"|\" 연산자를 이용하여 Runnable Sequence chain 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P0cahXhesl-E",
    "outputId": "eed661f4-9190-4da5-f8df-0139d0df7b64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = summarize_prompt | llm | output_parser\n",
    "\n",
    "print(type(chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "id": "WpJAZmpOsjHA",
    "outputId": "147f9a53-6d84-4d3c-cae6-4322635e23f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain에 대한 포괄적인 설명을 제공하겠습니다:\\n\\nLangChain 핵심 개념:\\n1. 정의\\n- 대규모 언어 모델(LLM)을 활용한 애플리케이션 개발을 위한 오픈소스 프레임워크\\n- Python과 JavaScript로 구현 가능\\n\\n2. 주요 특징\\n- LLM 기반 애플리케이션 개발 간소화\\n- 복잡한 AI 워크플로우 구현 지원\\n- 다양한 모델과 도구 통합\\n\\n3. 주요 구성 요소\\n- Prompt 관리\\n- 메모리 관리\\n- 데이터 색인 및 검색\\n- 에이전트 및 체인 생성\\n- 외부 도구 연결\\n\\n4. 주요 활용 사례\\n- 대화형 AI 챗봇\\n- 문서 분석\\n- 데이터 요약\\n- 질의응답 시스템\\n- 개인화된 AI 애플리케이션\\n\\n5. 장점\\n- 유연성\\n- 확장성\\n- 모듈형 아키텍처\\n- 다양한 LLM 통합\\n\\n이는 LangChain의 핵심 개념을 간략하게 요약한 설명입니다.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"context\": \"LangChain 에 대해 설명해 줘.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_VLKGZ1HO7d"
   },
   "source": [
    "### RunnableLambda 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oI9MZ0sasxQf",
    "outputId": "c5356d5c-6811-43af-9bfb-31fe88e2638e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n",
      "<class 'langchain_core.runnables.base.RunnableLambda'>\n",
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "# RunnableLambda를 사용하여 Python 함수를 체인에 삽입합니다.\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "summarize_chain = summarize_prompt | llm | output_parser\n",
    "print(type(summarize_chain))\n",
    "\n",
    "# 사용자 정의 람다 함수를 정의하고 이를 RunnableLambda에 래핑합니다.\n",
    "length_lambda = RunnableLambda(lambda summary: f\"요약된 길이: {len(summary)} 글자\")\n",
    "print(type(length_lambda))\n",
    "\n",
    "lambda_chain = summarize_chain | length_lambda\n",
    "print(type(lambda_chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "TAa1W6iVHLnq",
    "outputId": "5b181616-a4f7-49a1-b807-9795fc389b9c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'요약된 길이: 325 글자'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_chain.invoke({\"context\": \"LangChain 에 대해 설명해 줘\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DhIbYeOgIYyi",
    "outputId": "6ee972ca-751b-4b86-a8cd-fefe6fbe8c41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context'] input_types={} partial_variables={} template='\\n    당신은 AI 개념을 요약하는 유용한 조수입니다.\\n    {context}\\n    context를 한국어로 요약해 주세요.\\n'\n",
      "client=<openai.resources.chat.completions.Completions object at 0x000001AA4D1F8FD0> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001AA4D202EB0> root_client=<openai.OpenAI object at 0x000001AA4B3EDAF0> root_async_client=<openai.AsyncOpenAI object at 0x000001AA4D1F8FA0> model_name='gpt-4o-mini' model_kwargs={} openai_api_key=SecretStr('**********')\n",
      "\n",
      "RunnableLambda(lambda summary: f'요약된 길이: {len(summary)} 글자')\n"
     ]
    }
   ],
   "source": [
    "for step in lambda_chain.steps:\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 체인이 달린 도구 사용  \n",
    "\n",
    "- 도구는 에이전트, 체인 또는 대형 언어 모델(LLM)이 세상과 상호 작용할 수 있게 하는 인터페이스입니다.\n",
    "    - Python REPL, Wikipdedia, YouTube, Zapier, Gradio, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RkuqKhVpyKxm"
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade -q youtube_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ZF5O2LeB-vD",
    "outputId": "86cbdc7a-8466-410b-864b-7c67acc519db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.youtube.com/watch?v=gKCklJOWB6o&pp=ygUP7L2U65Sp7ZWY64qUIEFJ', 'https://www.youtube.com/watch?v=O5-cNXWWdzs&pp=ygUP7L2U65Sp7ZWY64qUIEFJ']\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import YouTubeSearchTool\n",
    "\n",
    "# YouTubeSearchTool 인스턴스 생성\n",
    "youtube_tool = YouTubeSearchTool()\n",
    "\n",
    "# 아무 키워드로 유튜브 검색 실행\n",
    "results = youtube_tool.run(\"코딩하는 AI\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S5BZ28LTAtul",
    "outputId": "62f913a7-6b6c-415c-d494-4ad127a6a18d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_KNAlnEhJgROgTafqk8jMOIzF', 'function': {'arguments': '{\"query\":\"코딩하는 AI\"}', 'name': 'youtube_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 94, 'total_tokens': 112, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-f3e81846-9c98-4b58-9f7e-9f06d66cc1c2-0', tool_calls=[{'name': 'youtube_search', 'args': {'query': '코딩하는 AI'}, 'id': 'call_KNAlnEhJgROgTafqk8jMOIzF', 'type': 'tool_call'}], usage_metadata={'input_tokens': 94, 'output_tokens': 18, 'total_tokens': 112, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YouTube 도구를 LLM에 바인딩하여 parameter 정보를 알아냄\n",
    "llm_with_tools = llm.bind_tools([youtube_tool])\n",
    "\n",
    "msg = llm_with_tools.invoke(\"코딩하는 AI\")\n",
    "msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jHYOhC0BCPPe",
    "outputId": "7dd07d3c-7316-4d98-849d-5e9e025e67ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'youtube_search',\n",
       "  'args': {'query': '코딩하는 AI'},\n",
       "  'id': 'call_KNAlnEhJgROgTafqk8jMOIzF',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llm_with_tools 에서 parameter 정보를 알아낸다\n",
    "msg.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': '코딩하는 AI'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg.tool_calls[0][\"args\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S-D_-T27Azsy",
    "outputId": "87aec236-a536-48f7-8af1-bbbc8fd8f656"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001AA4D1F8FD0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001AA4D202EB0>, root_client=<openai.OpenAI object at 0x000001AA4B3EDAF0>, root_async_client=<openai.AsyncOpenAI object at 0x000001AA4D1F8FA0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'youtube_search', 'description': 'search for youtube videos associated with a person. the input to this tool should be a comma separated list, the first part contains a person name and the second a number that is the maximum number of video results to return aka num_results. the second part is optional', 'parameters': {'properties': {'query': {'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}, config={}, config_factories=[])\n",
       "| RunnableLambda(lambda x: x.tool_calls[0]['args'])\n",
       "| YouTubeSearchTool()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llm_with_tools에서 추출된 인수로 \n",
    "# YouTubeSearchTool을 사용하여 유튜브 검색 실행한는 chain 생성\n",
    "chain = llm_with_tools | (lambda x: x.tool_calls[0][\"args\"]) | youtube_tool\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke(\"코딩하는 AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gDYHTp3LIzr4",
    "outputId": "48b727f3-bd82-4602-ebdd-0a257fa76799"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.youtube.com/watch?v=gKCklJOWB6o&pp=ygUP7L2U65Sp7ZWY64qUIEFJ',\n",
       " 'https://www.youtube.com/watch?v=sf52u9ZvKzk&pp=ygUP7L2U65Sp7ZWY64qUIEFJ']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "ast.literal_eval(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chains 종류\n",
    "\n",
    "**LangChain**을 사용하여 다양한 체인(Chain) 및 LLM(대규모 언어 모델) 기반 애플리케이션을 구축합니다.\n",
    "\n",
    "**1. Simple Chain (단일 체인)**  \n",
    "- 하나의 프롬프트를 통해 LLM(OpenAI)을 사용하여 텍스트를 생성합니다.  \n",
    "\n",
    "\n",
    "**2. Simple Sequential Chain (연속 체인)**  \n",
    "-  여러 LLM 호출을 연속적으로 수행하여 출력을 다음 입력으로 전달합니다.  \n",
    "\n",
    "**3. Document 요약 체인**  \n",
    "- **목적:** 텍스트 문서를 요약합니다.  \n",
    "\n",
    "**4. 텍스트를 Vector Store로 변환**  \n",
    "- **4.1 VectortstoreIndexCreator**   \n",
    "- **4.2 Chroma DB 사용**  \n",
    "\n",
    "**5. HTTP Request Chain (웹 요청 체인)**  \n",
    "- HTTP 요청을 통해 외부 웹 데이터에서 정보를 추출합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Simple Chain (단일 체인)\n",
    "- 가장 기본적인 유형의 체인\n",
    "- 입력 프롬프트를 수신하고 이를 사용하여 텍스트를 생성하는 역할을 담당하는 언어 모델(LLM) 하나만 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국에는 다양한 매력을 가진 관광지가 많습니다. 다음은 한국에서 가볼 만한 몇 가지 추천 장소입니다.\n",
      "\n",
      "1. **서울**: 한국의 수도로, 경복궁, 창덕궁, 명동, 인사동, 남산타워 등 다양한 역사적 장소와 현대적인 쇼핑, 맛집이 가득합니다.\n",
      "\n",
      "2. **부산**: 해운대, 광안리 해수욕장, 자갈치 시장 등 바다와 함께 즐길 수 있는 도시입니다. 또한 태종대와 감천문화마을도 인기 있는 관광지입니다.\n",
      "\n",
      "3. **제주도**: 아름다운 자연 경관과 독특한 문화가 어우러진 곳입니다. 한라산, 성산일출봉, 만장굴, 우도 등 다양한 명소가 있습니다.\n",
      "\n",
      "4. **경주**: 신라의 고도로, 불국사와 석굴암, 경주 역사 유적지구 등 유네스코 세계문화유산이 많아 역사와 문화를 체험할 수 있습니다.\n",
      "\n",
      "5. **전주**: 한옥마을로 유명하며, 전통 음식인 비빔밥과 막걸리를 맛볼 수 있는 곳입니다. 전통문화 체험도 가능합니다.\n",
      "\n",
      "6. **강릉**: 아름다운 해변과 커피 거리로 유명한 도시입니다. 경포대와 오죽헌도 방문할 만합니다.\n",
      "\n",
      "이 외에도 한국에는 다양한 매력을 가진 지역이 많으니, 여행의 목적에 맞춰 선택해 보세요!\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\"{place}에서 가장 가 볼만한 곳은?\")\n",
    "\n",
    "chain = prompt | llm\n",
    "# chain = prompt.pipe(llm)\n",
    "\n",
    "# 입력 변수만 지정하여 체인을 실행\n",
    "print(chain.invoke(\"한국\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Simple Sequential Chains (연속 체인)\n",
    "- Sequential Chain은 언어 모델에 대한 일련의 연속 호출 포함\n",
    "- 이 접근 방식은 한 호출에서 생성된 출력을 다른 호출의 입력으로 활용할 때 특히 유용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['place'], input_types={}, partial_variables={}, template='{place}에서 방문하기 가장 좋은 장소 5곳을 추천해주세요\\n\\n응답:\\n')\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001AA4D1F8FD0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001AA4D202EB0>, root_client=<openai.OpenAI object at 0x000001AA4B3EDAF0>, root_async_client=<openai.AsyncOpenAI object at 0x000001AA4D1F8FA0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| PromptTemplate(input_variables=['budget'], input_types={}, partial_variables={}, template='장소 목록이 주어지면, 모든 장소를 방문하는 데 드는 비용과 방문에 필요한 날짜를 현지 통화로 추산해 주십시오. \\n그리고 나서 예산 {budget}과 비교하여 충분한지, 부족한지 계산해 주세요.\\n\\n응답:\\n')\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001AA4D1F8FD0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001AA4D202EB0>, root_client=<openai.OpenAI object at 0x000001AA4B3EDAF0>, root_async_client=<openai.AsyncOpenAI object at 0x000001AA4D1F8FA0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_1 = \"\"\"{place}에서 방문하기 가장 좋은 장소 5곳을 추천해주세요\n",
    "\n",
    "응답:\n",
    "\"\"\"\n",
    "\n",
    "prompt1 = PromptTemplate.from_template(template_1)\n",
    "\n",
    "chain1 = prompt1 | llm\n",
    "\n",
    "template_2 = \"\"\"장소 목록이 주어지면, 모든 장소를 방문하는 데 드는 비용과 방문에 필요한 날짜를 현지 통화로 추산해 주십시오. \n",
    "그리고 나서 예산 {budget}과 비교하여 충분한지, 부족한지 계산해 주세요.\n",
    "\n",
    "응답:\n",
    "\"\"\"\n",
    "\n",
    "prompt2 = PromptTemplate.from_template(template_2)\n",
    "\n",
    "chain2 = prompt2 | llm\n",
    "\n",
    "final_chain = chain1 | chain2\n",
    "final_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "장소 목록을 기반으로 각 장소를 방문하는 데 드는 대략적인 비용과 필요한 날짜를 추산해 보겠습니다. 아래는 한국에서 추천한 5곳의 장소에 대한 정보입니다.\n",
      "\n",
      "### 1. 서울\n",
      "- **경복궁**\n",
      "  - **비용**: 입장료 약 3,000원 (약 $2.5)\n",
      "  - **소요 시간**: 2시간\n",
      "- **홍대**\n",
      "  - **비용**: 식사 및 카페 약 15,000원 (약 $12)\n",
      "  - **소요 시간**: 3시간\n",
      "\n",
      "### 2. 부산\n",
      "- **해운대**\n",
      "  - **비용**: 해변 이용 무료, 식사 및 카페 약 20,000원 (약 $16)\n",
      "  - **소요 시간**: 4시간\n",
      "- **광안리**\n",
      "  - **비용**: 해변 이용 무료, 액티비티 비용 약 30,000원 (약 $25)\n",
      "  - **소요 시간**: 3시간\n",
      "\n",
      "### 3. 제주도\n",
      "- **한라산**\n",
      "  - **비용**: 입장료 약 2,000원 (약 $1.5), 교통비 포함 약 25,000원 (약 $20)\n",
      "  - **소요 시간**: 6시간\n",
      "- **성산 일출봉**\n",
      "  - **비용**: 입장료 약 2,000원 (약 $1.5)\n",
      "  - **소요 시간**: 2시간\n",
      "\n",
      "### 4. 경주\n",
      "- **불국사**\n",
      "  - **비용**: 입장료 약 5,000원 (약 $4)\n",
      "  - **소요 시간**: 3시간\n",
      "- **경주 역사유적지구**\n",
      "  - **비용**: 무료, 교통비 약 10,000원 (약 $8)\n",
      "  - **소요 시간**: 3시간\n",
      "\n",
      "### 5. 전주\n",
      "- **전주 한옥마을**\n",
      "  - **비용**: 무료, 전통 음식 약 15,000원 (약 $12)\n",
      "  - **소요 시간**: 4시간\n",
      "\n",
      "### 총 비용 및 소요 날짜\n",
      "- **총 비용**: \n",
      "  - 서울: 18,000원 (약 $15)\n",
      "  - 부산: 50,000원 (약 $41)\n",
      "  - 제주도: 29,000원 (약 $24)\n",
      "  - 경주: 15,000원 (약 $12)\n",
      "  - 전주: 15,000원 (약 $12)\n",
      "\n",
      "**총합**: 127,000원 (약 $104)\n",
      "\n",
      "- **총 소요 시간**: \n",
      "  - 서울: 5시간\n",
      "  - 부산: 7시간\n",
      "  - 제주도: 8시간\n",
      "  - 경주: 6시간\n",
      "  - 전주: 4시간\n",
      "\n",
      "**총합**: 30시간\n",
      "\n",
      "### 결론\n",
      "- **총 비용**: 127,000원 (약 $104)\n",
      "- **총 소요 시간**: 30시간\n",
      "\n",
      "여기에 대한 예산이 충분한지 부족한지는 개인의 여행 경비와 계획에 따라 다르겠지만, 대체로 한국의 주요 관광지를 방문하는 데 있어 합리적인 예산으로 보입니다. 추가적인 경비(숙박, 교통비 등)를 고려하면 더 많은 예산이 필요할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "review = final_chain.invoke({\"place\": \"한국\", \"budget\": \"1,000,000\"})\n",
    "print(review.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
